{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating metadata\n",
    "\n",
    "### What this notebook does\n",
    "**Step 1:** Create metadata file, containing a bunch of JSON-formatted trial metadata objects\n",
    "\n",
    "**Step 2:**  Insert each trial as a record into a mongo database\n",
    "\n",
    "This assumes that the stimuli have been uploaded to the S3 bucket using `upload_stims_to_s3.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymongo as pm\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import ast\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(paths, ext='mp4'):\n",
    "    \"\"\"Pass list of folders if there are stimuli in multiple folders. \n",
    "    Make sure that the containing folder is informative, as the rest of the path is ignored in naming. \n",
    "    Also returns filenames as uploaded to S3\"\"\"\n",
    "    if type(paths) is not list:\n",
    "        paths = [paths]\n",
    "    results = []\n",
    "    names = []\n",
    "    for path in paths:\n",
    "        results += [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.%s' % ext))]\n",
    "        names += [os.path.basename(os.path.dirname(y))+'_'+os.path.split(y)[1] for x in os.walk(path) for y in glob(os.path.join(x[0], '*.%s' % ext))]\n",
    "    return results,names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## where are your stimulus images stored?\n",
    "data_dirs = ['./pilot_dominoes'] #Where on disk are the stimuli stored?\n",
    "bucket_name = 'human-physics-benchmarking-dominoes-pilot'\n",
    "stim_version = 'example'\n",
    "dataset_name = '{}_{}'.format(bucket_name, stim_version)\n",
    "stimulus_extension = \"mp4\" #what's the file extension for the stims? Provide without dot\n",
    "\n",
    "## get a list of paths to each one\n",
    "full_stim_paths,filenames = list_files(data_dirs,stimulus_extension)\n",
    "print('We have {} stimuli to evaluate.'.format(len(full_stim_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to have a number of familiarization trials. Put a couple of hand selected filenames here. The stims are expected in the S3 bucket with that filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "familiarization_filenames = ['pilot_dominoes_SJ025_boxroom_0000_img.mp4\t', \n",
    "                             'pilot_dominoes_SJ025_boxroom_0001_img.mp4\t',\n",
    "                             'pilot_dominoes_SJ025_boxroom_0002_img.mp4\t'\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper to build stim urls\n",
    "def build_s3_url(filename, bucket_name):    \n",
    "    return 'https://{}.s3.amazonaws.com/{}'.format(bucket_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic metadata lists\n",
    "stim_urls = [build_s3_url(p,bucket_name) for p in filenames]\n",
    "stim_IDs = [name.split('.')[0] for name in filenames]\n",
    "familiarization_stim_urls = [build_s3_url(p,bucket_name) for p in familiarization_filenames]\n",
    "familiarization_stim_IDs = [name.split('.')[0] for name in familiarization_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to pandas dataframe\n",
    "M = pd.DataFrame([stim_IDs,stim_urls]).transpose()\n",
    "M.columns = ['stim_ID', 'stim_url']\n",
    "familiarization_M = pd.DataFrame([familiarization_stim_IDs,familiarization_stim_urls]).transpose()\n",
    "familiarization_M.columns = ['stim_ID', 'stim_url']\n",
    "# save the familiariaziation dict\n",
    "familiarization_trials = familiarization_M.transpose().to_dict()\n",
    "# needs to have strings as keys\n",
    "familiarization_trials = {str(key):value for key, value in familiarization_trials.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, add code to add additional columns\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "familiarization_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now have a list of all the stimuli to evaluate. Let's create random orders of them and submit them to the database. Set the number of stimuli shown to one participant and the number of different orders to generate (`num_of_permutations`). Each of these sets can be shown to a participant, with the least often shown set being shown (so we can get away with fewer sets than subjects). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stims_per_run = len(M) #len(M) to show all stimuli\n",
    "num_of_sets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of stimuli as dicts\n",
    "L = M.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of lists of shuffled, sampled w/o replacement stimuli\n",
    "stim_sets = []\n",
    "for i in range(num_of_sets):\n",
    "    stim_set = random.sample(L, stims_per_run)\n",
    "    stim_sets += [{str(num):stim for num,stim in enumerate(stim_set)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stim_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will be our familiarization trials\n",
    "familiarization_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of sequences of stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ssh bridge to write to mongodb. Insert your username. If you don't have an SSH secret set yet, run `ssh -fNL 27017:127.0.0.1:27017 USERNAME@cogtoolslab.org` in your shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh -fNL 27017:127.0.0.1:27017 fbinder@cogtoolslab.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('../auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user. Place it in the toplevel of the repo\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org' ## cogtoolslab ip address\n",
    "\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⚠️drop collection if necessary. \n",
    "db.drop_collection(dataset_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of current collections\n",
    "sorted(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## actually add data now to the database\n",
    "for (i,m) in enumerate(stim_sets):\n",
    "    coll.insert_one({'stims':m, 'familiarization_stims': familiarization_trials})\n",
    "    print('{} of {}'.format(i+1, len(J)))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print('Done inserting records into mongo! The collection name is',dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(coll.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xeus-python]",
   "language": "python",
   "name": "conda-env-xeus-python-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
