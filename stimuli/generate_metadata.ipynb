{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating metadata\n",
    "\n",
    "### What this notebook does\n",
    "**Step 1:** Create metadata file, containing a bunch of JSON-formatted trial metadata objects\n",
    "\n",
    "**Step 2:**  Insert each trial as a record into a mongo database\n",
    "\n",
    "This assumes that the stimuli have been uploaded to the S3 bucket using `upload_stims_to_s3.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Which experiment? bucket_name is the name of the experiment and will be name of the databases both on mongoDB and S3\n",
    "bucket_name = 'human-physics-benchmarking-XXX-pilot' #CHANGE THIS ⚡️\n",
    "stim_version = 'iteration_2' #CHANGE THIS ⚡️\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymongo as pm\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import ast\n",
    "import itertools\n",
    "import random\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set random number state\n",
    "rng = np.random.RandomState(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_files(paths, ext='mp4'):\n",
    "    \"\"\"Pass list of folders if there are stimuli in multiple folders. \n",
    "    Make sure that the containing folder is informative, as the rest of the path is ignored in naming. \n",
    "    Also returns filenames as uploaded to S3\"\"\"\n",
    "    if type(paths) is not list:\n",
    "        paths = [paths]\n",
    "    results = []\n",
    "    names = []\n",
    "    for path in paths:\n",
    "        results += [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.%s' % ext))]\n",
    "        names += [os.path.basename(os.path.dirname(y))+'_'+os.path.split(y)[1] for x in os.walk(path) for y in glob(os.path.join(x[0], '*.%s' % ext))]\n",
    "    hdf5s = [r.split(\"_img.\")[0]+\".hdf5\" for r in results]\n",
    "    return results,names,hdf5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_stem = 'XXX' #CHANGE THIS ⚡️\n",
    "dirnames = [d.split('/')[-1] for d in glob(local_stem+'/*')]\n",
    "data_dirs = [local_stem + d for d in dirnames]\n",
    "\n",
    "dataset_name = '{}_{}'.format(bucket_name, stim_version)\n",
    "stimulus_extension = \"mp4\" #what's the file extension for the stims? Provide without dot\n",
    "\n",
    "## get a list of paths to each one\n",
    "full_stim_paths,filenames, _  = list_files(data_dirs,stimulus_extension)\n",
    "full_map_paths, mapnames, _ = list_files(data_dirs, ext = 'png') #generate filenames and stimpaths for target/zone map\n",
    "full_hdf5_paths,hdf5_names, _ = list_files(data_dirs,ext = 'hdf5')\n",
    "print('We have {} stimuli to evaluate.'.format(len(full_stim_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert all('_img' in p for p in full_stim_paths), \"Make sure to only pass in `_img` passes!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to have a number of familiarization trials. Put a couple of hand selected filenames here. The stims are expected in the S3 bucket with that filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiarization_stem = 'XXX' #CHANGE THIS ⚡️\n",
    "\n",
    "NUM_FAM_TRIALS = 10\n",
    "\n",
    "order = list(rng.permutation(list(range(NUM_FAM_TRIALS))))\n",
    "\n",
    "try:\n",
    "    familiarization_filenames = [familiarization_stem + ('_%04d_img.mp4' % d) \n",
    "                             for d in order]\n",
    "    familiarization_mapnames = [familiarization_stem + ('_%04d_map.png' % d)\n",
    "                             for d in order]    \n",
    "except IndexError:\n",
    "    familiarization_filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## helper to build stim urls\n",
    "def build_s3_url(filename, bucket_name):    \n",
    "    return 'https://{}.s3.amazonaws.com/{}'.format(bucket_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## basic metadata lists\n",
    "stim_urls = [build_s3_url(p,bucket_name) for p in filenames]\n",
    "stim_map_urls = [build_s3_url(p,bucket_name) for p in mapnames]\n",
    "stim_IDs = [name.split('.')[0] for name in filenames]\n",
    "stim_hdf5_urls = [build_s3_url(p,bucket_name) for p in hdf5_names]\n",
    "hdf5_paths = ['/'.join(p.split('/')[-2:]) for p in full_hdf5_paths]\n",
    "familiarization_stim_urls = [build_s3_url(p,bucket_name) for p in familiarization_filenames]\n",
    "familiarization_map_urls = [build_s3_url(p,bucket_name) for p in familiarization_mapnames]\n",
    "familiarization_stim_IDs = [name.split('.')[0] for name in familiarization_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(stim_IDs),len(familiarization_stim_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## convert to pandas dataframe\n",
    "M = pd.DataFrame([stim_IDs,stim_urls, stim_map_urls, hdf5_paths, stim_hdf5_urls]).transpose()\n",
    "M.columns = ['stim_ID', 'stim_url', 'map_url', 'hdf5_path','hdf5_url']\n",
    "\n",
    "## drop the stims that have the same stem as the familiarization trials\n",
    "M = M[~M['stim_ID'].isin([n.split('.')[0] for n in familiarization_filenames])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiarization_M = pd.DataFrame([familiarization_stim_IDs,familiarization_stim_urls, familiarization_map_urls]).transpose()\n",
    "familiarization_M.columns = ['stim_ID', 'stim_url', 'map_url']\n",
    "# save the familiariaziation dict\n",
    "familiarization_trials = familiarization_M.transpose().to_dict()\n",
    "# needs to have strings as keys\n",
    "familiarization_trials = {str(key):value for key, value in familiarization_trials.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(M),len(familiarization_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove some bad stimuli -- regenerate these\n",
    "bad_stimuli = [\n",
    "    #filename of bad stimuli #CHANGE THIS IF NEEDED ⚡️\n",
    "]\n",
    "\n",
    "for nm in bad_stimuli:\n",
    "    M = M[~M['stim_ID'].str.contains(nm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add metadata to the stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `metadata.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if needed, add code to add additional columns\n",
    "# Add trial labels to the metadata using the stimulus metadata.json\n",
    "target_hit_zone_labels = dict()\n",
    "for _dir in data_dirs:\n",
    "    with open(_dir + '/metadata.json', 'rb') as f:\n",
    "        trial_metas = json.load(f)\n",
    "        \n",
    "    for i,meta in enumerate(trial_metas):\n",
    "        stim_name = meta['stimulus_name']\n",
    "        if stim_name == 'None': #recreate stimname from order in metadata\n",
    "            stim_name = str(i).zfill(4)\n",
    "            stim_name = _dir.split('/')[-1] + '_' + stim_name\n",
    "        if stim_name[-4:] != \"_img\": stim_name+='_img' #stimnames need to end in \"_img\"\n",
    "        label = meta['does_target_contact_zone']\n",
    "        target_hit_zone_labels[stim_name] = label\n",
    "        \n",
    "print(\"num positive labels: %d\" % sum(list(target_hit_zone_labels.values())))\n",
    "print(\"num negative labels: %d\" % (len(target_hit_zone_labels) - sum(list(target_hit_zone_labels.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make new df with all metadata\n",
    "GT = pd.DataFrame([list(target_hit_zone_labels.keys()), list(target_hit_zone_labels.values())]).transpose()\n",
    "GT.columns = ['stim_ID', 'target_hit_zone_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sanity check—this should be empty save for maybe leftover familiarization stims\n",
    "set(M['stim_ID']).symmetric_difference(GT['stim_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge with M\n",
    "# if all([col not in M.columns for col in GT.columns[1:]]):\n",
    "M = M.merge(GT, on='stim_ID')\n",
    "print(\"added labels %s\" % list(GT.columns[1:]))\n",
    "    \n",
    "# if all([col not in familiarization_M.columns for col in GT.columns[1:]]):\n",
    "fam_keys = [k for k in list(target_hit_zone_labels.keys()) if (k+'.mp4') in familiarization_filenames]\n",
    "fam_labels = [target_hit_zone_labels[k] for k in fam_keys]\n",
    "familiarization_GT = pd.DataFrame([fam_keys, fam_labels]).transpose()\n",
    "familiarization_GT.columns = ['stim_ID', 'target_hit_zone_label']\n",
    "familiarization_M = familiarization_M.merge(familiarization_GT, on='stim_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that we have the same amount of stimuli. To prevent data from being too sparse, we also might want to sample a subset of stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STIM_SET_SIZE = 150 #how many total stimuli do we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_label_num = min(len(M[M['target_hit_zone_label'] == False]),\n",
    "                   len(M[M['target_hit_zone_label'] == True]),\n",
    "                   int(STIM_SET_SIZE/2))\n",
    "print(\"We get\", per_label_num, \"stimuli per label for a total of\", per_label_num * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_rows = M[M['target_hit_zone_label'] == True].to_dict(orient='records')\n",
    "positive_rows = list(rng.choice(positive_rows,per_label_num))\n",
    "negative_rows = M[M['target_hit_zone_label'] == False].to_dict(orient='records')\n",
    "negative_rows = list(rng.choice(negative_rows,per_label_num))\n",
    "all_rows = positive_rows + negative_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# recreate M\n",
    "M = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets save the experimental stims out into a json\n",
    "M.to_json(bucket_name+'_'+stim_version+'_experimental_stims'+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiarization_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiarization_GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `hdf5s` get all the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {} #holds all the metadata for all stimuli\n",
    "\n",
    "for name,hdf5_path in zip([f.split('.')[0] for f in filenames],full_hdf5_paths):\n",
    "    #load hdf5\n",
    "#     print(\"loading\",hdf5_path)\n",
    "    try:\n",
    "        hdf5 = h5py.File(hdf5_path,'r') #get the static part of the HDF5\n",
    "        stim_name = str(np.array(hdf5['static']['stimulus_name']))\n",
    "        metadatum = {} #metadata for the current stimulus\n",
    "        for key in hdf5['static'].keys():\n",
    "            datum = np.array(hdf5['static'][key])\n",
    "            if datum.shape == (): datum = datum.item() #unwrap non-arrays\n",
    "            metadatum[key] = datum\n",
    "        #close file\n",
    "        hdf5.close()\n",
    "        metadata[name] = metadatum\n",
    "    except Exception as e:\n",
    "        print(\"Error with\",hdf5_path,\":\",e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[list(metadata.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert those metadatas into M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in M.index:\n",
    "    stim_name = M.at[index,'stim_ID']\n",
    "    for key,value in metadata[stim_name].items():\n",
    "        M.at[index,key] = str(value) #insert every item as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now have a list of all the stimuli to evaluate. Let's create random orders of them and submit them to the database. Set the number of stimuli shown to one participant and the number of different orders to generate (`num_of_permutations`). Each of these sets can be shown to a participant, with the least often shown set being shown (so we can get away with fewer sets than subjects). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stims_per_run = len(M) #len(M) to show all stimuli\n",
    "num_of_sets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate list of stimuli as dicts\n",
    "L = M.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate list of lists of shuffled, sampled w/o replacement stimuli\n",
    "stim_sets = []\n",
    "for i in range(num_of_sets):\n",
    "    stim_set = list(rng.choice(L, stims_per_run))\n",
    "    stim_sets += [{str(num):stim for num,stim in enumerate(stim_set)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(stim_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiarization_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will be our familiarization trials\n",
    "familiarization_trials = familiarization_M.transpose().to_dict()\n",
    "familiarization_trials = {str(key):value for key, value in familiarization_trials.items()}\n",
    "familiarization_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of sequences of stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ssh bridge to write to mongodb. Insert your username. If you don't have an SSH secret set yet, run `ssh -fNL 27017:127.0.0.1:27017 USERNAME@cogtoolslab.org` in your shell. \\\n",
    "*CHANGE THIS ⚡️*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh -fNL 27017:127.0.0.1:27017 XXX@cogtoolslab.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('../auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user. Place it in the toplevel of the repo\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org' ## cogtoolslab ip address\n",
    "\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⚠️drop collection if necessary. \n",
    "db.drop_collection(dataset_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of current collections\n",
    "sorted(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **do it**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets save the experiment structure out into a json as well\n",
    "#rows are different games, columns are the full metadata for the nth stimulus\n",
    "pd.DataFrame(stim_sets).to_json(bucket_name+'_'+stim_version+'_experiment'+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## actually add data now to the database\n",
    "for (i,m) in enumerate(stim_sets):\n",
    "    coll.insert_one({'stims':m, 'familiarization_stims': familiarization_trials})\n",
    "    print('{} of {}'.format(i+1, len(stim_sets)))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print('Done inserting records into mongo! The collection name is',dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(coll.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tdw]",
   "language": "python",
   "name": "conda-env-tdw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
