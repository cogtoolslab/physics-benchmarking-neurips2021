{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating metadata\n",
    "\n",
    "### What this notebook does\n",
    "**Step 1:** Create metadata file, containing a bunch of JSON-formatted trial metadata objects\n",
    "\n",
    "**Step 2:**  Insert each trial as a record into a mongo database\n",
    "\n",
    "This assumes that the stimuli have been uploaded to the S3 bucket using `upload_stims_to_s3.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymongo as pm\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## where are your stimulus images stored?\n",
    "data_dir = 'example'\n",
    "bucket_name = 'human-physics-benchmarking-pilot'\n",
    "stim_version = 'example'\n",
    "dataset_name = '{}_{}'.format(bucket_name, stim_version)\n",
    "stimulus_extension = \".mp4\" #what's the file extension for the stims?\n",
    "\n",
    "## get a list of paths to each one\n",
    "full_stim_paths = [os.path.abspath(os.path.join(data_dir,i)) for i in os.listdir(data_dir) if stimulus_extension in i]\n",
    "print('We have {} stimuli to evaluate.'.format(len(full_stim_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper to build image urls\n",
    "def build_s3_url(path, bucket_name):    \n",
    "    return 'https://{}.s3.amazonaws.com/{}'.format(bucket_name, path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic metadata lists\n",
    "stim_urls = [build_s3_url(p,bucket_name) for p in full_stim_paths]\n",
    "stim_IDs = [p.split('/')[-1].split('.')[0] for p in full_stim_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to pandas dataframe\n",
    "M = pd.DataFrame([stim_IDs,stim_urls]).transpose()\n",
    "M.columns = ['stim_ID', 'stim_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, add code to add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = M.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving out json dictionary out to file...') \n",
    "with open('{}.js'.format(dataset_name), 'w') as fout:\n",
    "    json.dump(L, fout)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ssh bridge to write to mongodb. Insert your username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh -fNL 27017:127.0.0.1:27017 fbinder@cogtoolslab.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('../auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user. Place it in the toplevel of the repo\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org' ## cogtoolslab ip address\n",
    "\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload JSON back in to the new stimulus collection\n",
    "J = json.loads(open('{}.js'.format(dataset_name),mode='r').read())\n",
    "print('dataset_name: {}'.format(dataset_name))\n",
    "print('Length of J is: {}'.format(len(J)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⚠️drop collection if necessary\n",
    "db.drop_collection(dataset_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of current collections\n",
    "sorted(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## actually add data now to the database\n",
    "for (i,m) in enumerate(J):\n",
    "    coll.insert_one(m)\n",
    "    print('{} of {}'.format(i+1, len(J)))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print('Done inserting records into mongo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xeus-python]",
   "language": "python",
   "name": "conda-env-xeus-python-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
