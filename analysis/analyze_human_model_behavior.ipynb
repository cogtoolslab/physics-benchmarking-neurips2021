{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of human and model behavior across physical domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to:** \n",
    "* Apply preprocessing to human behavioral data\n",
    "* Visualize distribution of human physical judgments, by domain\n",
    "* Compute summary statistics over human physical judgments, by domain\n",
    "* Visualize distribution of model physical judgments, by domain\n",
    "* Compute summary statistics over model physical judgments, by domain\n",
    "* Conduct human-model comparisons\n",
    "* Output CSV that can be re-loaded into R notebook for statistical modeling & fancy visualizations\n",
    "\n",
    "**This notebook depends on:**\n",
    "* Running `./generate_dataframes.py` (INTERNAL USE ONLY)\n",
    "* Running `./upload_results.py` (INTERNAL USE ONLY)\n",
    "* Running `./download_results.py` (PUBLIC USE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "\n",
    "from importlib import reload\n",
    "from utils import *\n",
    "\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../analysis/utils\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# seaborn plotting themes\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution of human physical judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>bucket_name</th>\n",
       "      <th>stim_version</th>\n",
       "      <th>iterationName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dominoes_pilot</td>\n",
       "      <td>human-physics-benchmarking-dominoes-pilot</td>\n",
       "      <td>production_1</td>\n",
       "      <td>production_1_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collision_pilot</td>\n",
       "      <td>human-physics-benchmarking-collision-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>towers_pilot</td>\n",
       "      <td>human-physics-benchmarking-towers-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linking_pilot</td>\n",
       "      <td>human-physics-benchmarking-linking-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>containment_pilot</td>\n",
       "      <td>human-physics-benchmarking-containment-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rollingsliding_pilot</td>\n",
       "      <td>human-physics-benchmarking-rollingsliding-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drop_pilot</td>\n",
       "      <td>human-physics-benchmarking-drop-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clothiness_pilot</td>\n",
       "      <td>human-physics-benchmarking-clothiness-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  study                                      bucket_name  \\\n",
       "0        dominoes_pilot        human-physics-benchmarking-dominoes-pilot   \n",
       "1       collision_pilot       human-physics-benchmarking-collision-pilot   \n",
       "2          towers_pilot          human-physics-benchmarking-towers-pilot   \n",
       "3         linking_pilot         human-physics-benchmarking-linking-pilot   \n",
       "4     containment_pilot     human-physics-benchmarking-containment-pilot   \n",
       "5  rollingsliding_pilot  human-physics-benchmarking-rollingsliding-pilot   \n",
       "6            drop_pilot            human-physics-benchmarking-drop-pilot   \n",
       "7      clothiness_pilot      human-physics-benchmarking-clothiness-pilot   \n",
       "\n",
       "   stim_version         iterationName  \n",
       "0  production_1  production_1_testing  \n",
       "1  production_2  production_2_testing  \n",
       "2  production_2  production_2_testing  \n",
       "3  production_2  production_2_testing  \n",
       "4  production_2  production_2_testing  \n",
       "5  production_2  production_2_testing  \n",
       "6  production_2  production_2_testing  \n",
       "7  production_2  production_2_testing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_meta import *\n",
    "HEM = pd.DataFrame(NEURIPS2021_EXPS) # HEM = \"human experiment metadata\"\n",
    "HEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get paths to all human response data\n",
    "data_paths = [os.path.join(csv_dir,'humans',i) for i in os.listdir(os.path.join(csv_dir,'humans'))]\n",
    "resp_paths = [i for i in data_paths if i.split('/')[-1].split('-')[0]=='human_responses']\n",
    "assert len(resp_paths)==8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in example dataframe\n",
    "exp_ind = 0\n",
    "d = pd.read_csv(resp_paths[exp_ind])\n",
    "\n",
    "## some utility vars\n",
    "colnames_with_variable_entries = [col for col in sorted(d.columns) if len(np.unique(d[col]))>1]\n",
    "# colnames = ['gameID','trialNum','prolificIDAnon','stim_ID','response','target_hit_zone_label','correct','choices','rt']\n",
    "colnames = ['gameID','trialNum','stim_ID','response','target_hit_zone_label','correct','choices','rt']\n",
    "\n",
    "## subset dataframe by colnames of interest\n",
    "D = d[colnames]\n",
    "\n",
    "## preprocess RTs (subtract 2500ms presentation time, log transform)\n",
    "D = D.assign(RT = D['rt'] - 2500) \n",
    "D = D.assign(logRT = np.log(D['RT']))\n",
    "D = D.drop(columns=['rt'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exclusion criteria (from `preregistration_neurips2021.md`)\n",
    " <!-- How will you determine which data points or samples (if any) to exclude from your analyses? How will outliers be handled? Will you use any awareness or attention check? -->\n",
    " Data from an entire experimental session will be excluded if the responses:\n",
    " * contain a sequence with unusually long streak, defined as occurring less than 2.5% of the time under random responding\n",
    " * contain a sequence of at least 24 trials alternating \"yes\" and \"no\" responses\n",
    " * are correct for fewer than 4 out of 10 familiarization trials (i.e., 30% correct or lower)\n",
    " * the mean accuracy for that participant is below 3 standard deviations below the median accuracy across all participants for that scenario\n",
    " * the mean log-transformed response time for that participant is 3 standard deviations above the median log-transformed response time across all participants for that scenario\n",
    " \n",
    "Excluded sessions will be flagged. Flagged sessions will not be included in the main analyses. We will also conduct our planned analyses with the flagged sessions included to investigate the extent to which the outcomes of the main analyses change when these sessions are included. Specifically, we will fit a statistical model to all sessions and estimate the effect of a session being flagged on accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init flaggedIDs var\n",
    "flaggedIDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.5th percentile for streak length is 12.0.\n"
     ]
    }
   ],
   "source": [
    "## what is 97.5th percentile for random sequences of length numTrials and p=0.5?\n",
    "thresh = get_streak_thresh(150, 0.5)\n",
    "print('97.5th percentile for streak length is {}.'.format(thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 flagged IDs so far due to long streaks.\n"
     ]
    }
   ],
   "source": [
    "## flag sessions with long streaks\n",
    "streakyIDs = []\n",
    "for name, group in D.groupby('gameID'):\n",
    "    seq = group['response'].values\n",
    "    streak_length = get_longest_streak_length(group['response'].values)\n",
    "    if streak_length>thresh:\n",
    "        streakyIDs.append(name)\n",
    "print('There are {} flagged IDs so far due to long streaks.'.format(len(streakyIDs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 flagged IDs so far due to alternating sequences.\n"
     ]
    }
   ],
   "source": [
    "## flag sessions with suspicious alternation pattern\n",
    "alternatingIDs = []\n",
    "pattern = list(np.unique(D['response'].values))*10\n",
    "for name, group in D.groupby('gameID'):\n",
    "    seq = group['response'].values\n",
    "    substr = ''.join(pattern)\n",
    "    fullstr = ''.join(seq)\n",
    "    if substr in fullstr:\n",
    "        alternatingIDs.append(name)\n",
    "print('There are {} flagged IDs so far due to alternating sequences.'.format(len(alternatingIDs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: flag familiarization trial failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 flagged IDs so far due to low accuracy.\n"
     ]
    }
   ],
   "source": [
    "## flag sessions with unusually low accuracy\n",
    "Dacc = D.groupby('gameID').agg({'correct':np.mean})\n",
    "thresh = np.mean(Dacc['correct']) - 3*np.std(Dacc['correct'])\n",
    "Dacc = Dacc.assign(lowAcc = Dacc['correct']<thresh)\n",
    "lowAccIDs = list(Dacc[Dacc['lowAcc']==True].index)\n",
    "print('There are {} flagged IDs so far due to low accuracy.'.format(len(lowAccIDs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 flagged IDs so far due to high RTs.\n"
     ]
    }
   ],
   "source": [
    "## flag sessions with unusually high RTs\n",
    "Drt = D.groupby('gameID').agg({'logRT':np.median})\n",
    "thresh = np.median(Drt['logRT']) + 3*np.std(Drt['logRT'])\n",
    "Drt = Drt.assign(highRT = Drt['logRT']>thresh)\n",
    "highRTIDs = list(Drt[Drt['highRT']==True].index)\n",
    "print('There are {} flagged IDs so far due to high RTs.'.format(len(highRTIDs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 6 flagged IDs.\n"
     ]
    }
   ],
   "source": [
    "## combining all flagged sessions\n",
    "flaggedIDs = streakyIDs + alternatingIDs + lowAccIDs + highRTIDs\n",
    "print('There are a total of {} flagged IDs.'.format(len(np.unique(flaggedIDs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing flagged sessions from dataset\n",
    "D = D[~D.gameID.isin(flaggedIDs)]\n",
    "print('There are a total of {} valid and complete sessions.'.format(len(np.unique(D.gameID.values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute summary statistics over human physical judgments, by domain\n",
    "\n",
    "### Human accuracy across participants for each stimulus\n",
    "We will analyze accuracy for each stimulus by computing the proportion of correct responses across all participants who viewed that stimulus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy across stimuli for each participant\n",
    "We will analyze accuracy for each participant by computing the proportion of correct responses across all stimuli. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-human consistency for each stimulus\n",
    "We will estimate human-human consistency for each stimulus by computing the proportion of responses that match the modal response for that stimulus (whether that modal response is correct or incorrect).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-human consistency across stimuli (within scenario)\n",
    "We will analyze human-human consistency by computing the mean correlation between (binary) response vectors produced by each human participant across all stimuli within each scenario. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy as a function of stimulus attributes\n",
    "We will conduct exploratory analyses of human accuracy as a function of various scenario-specific stimulus attributes that varied across trials. We will examine those stimulus attributes that varied across stimuli within each scenario and explore the relationship between each individual attribute and human accuracy, as well as beetween linear combinations of them and human accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy by scenario\n",
    "We will fit human responses across all scenarios with a mixed-effects logistic regression model, including scenario as a fixed effect and participants and individual stimuli as random effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other exploratory human behavioral analyses\n",
    "* We will explore the relation of demographic variables on the performance of participants: how does age, gender, educational status and the the result of a one-trial spatial reasoning task relate to the overall accuracy of a subject?\n",
    "* We will additionally explore any potential left/right or yes/no response biases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution of model physical judgments, by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute summary statistics over model physical judgments, by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct human-model comparisons\n",
    "We will compare human and model behavior in two ways: **absolute performance** and **response pattern.**\n",
    "\n",
    "#### **Absolute Performance** \n",
    "We will compare the accuracy of each model to the mean accuracy of humans, for each scenario. \n",
    "To do this, we will first compute estimates of mean human accuracy for each scenario and construct 95% confidence intervals for each of these estimates. \n",
    "These confidence intervals will be constructed by bootstrapping: specifically, for an experiment with N participants, we will resample N participants with replacement and compute the proportion correct for that bootstrapped sample. We will take repeat this resampling procedure 1000 times to generate a sampling distribution for the mean proportion correct. The 2.5th and 97.5th percentile will be extracted from this sampling distribution to provide the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "For each model, we will then compare their proportion correct (a point estimate) to the human confidence interval. \n",
    "\n",
    "#### **Response Pattern**\n",
    "We will compare the pattern of predictions generated by each model to the pattern of predictions generated by humans. \n",
    "\n",
    "We will do this by using two standard inter-rater reliability metrics:\n",
    "##### **Correlation**\n",
    "For each pair of human participants, we will compute the correlation between their (binary) response vectors, yielding a distribution of pairwise human-human correlations. \n",
    "For each model, we will compute the correlation between its response vector and every human participant, as well as every other model. \n",
    "A model's response pattern will be considered more similar to humans' insofar as the mean model-human correlation (across humans) lies closer to the mean human-human correlation (for all pairs of humans).\n",
    "\n",
    "##### **Cohen's kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
