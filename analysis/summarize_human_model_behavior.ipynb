{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of human and model behavior across physical domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to:** \n",
    "* Apply preprocessing to human behavioral data\n",
    "* Visualize distribution and compute summary statistics over **human** physical judgments\n",
    "* Visualize distribution and compute summary statistics over **model** physical judgments\n",
    "* Conduct human-model comparisons\n",
    "* Output CSV that can be re-loaded into R notebook for statistical modeling & fancy visualizations\n",
    "\n",
    "**This notebook depends on:**\n",
    "* Running `./generate_dataframes.py` (INTERNAL USE ONLY)\n",
    "* Running `./upload_results.py` (INTERNAL USE ONLY)\n",
    "* Running `./download_results.py` (PUBLIC USE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:25:58.602229Z",
     "iopub.status.busy": "2021-06-03T22:25:58.601894Z",
     "iopub.status.idle": "2021-06-03T22:26:00.449403Z",
     "shell.execute_reply": "2021-06-03T22:26:00.448677Z",
     "shell.execute_reply.started": "2021-06-03T22:25:58.602146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "\n",
    "sys.path.append('./analysis_helpers')\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import analysis_helpers as h\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:00.451469Z",
     "iopub.status.busy": "2021-06-03T22:26:00.451110Z",
     "iopub.status.idle": "2021-06-03T22:26:00.455935Z",
     "shell.execute_reply": "2021-06-03T22:26:00.454737Z",
     "shell.execute_reply.started": "2021-06-03T22:26:00.451440Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# seaborn plotting themes\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:00.458097Z",
     "iopub.status.busy": "2021-06-03T22:26:00.457796Z",
     "iopub.status.idle": "2021-06-03T22:26:00.468869Z",
     "shell.execute_reply": "2021-06-03T22:26:00.467896Z",
     "shell.execute_reply.started": "2021-06-03T22:26:00.458070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(analysis_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(analysis_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:00.470590Z",
     "iopub.status.busy": "2021-06-03T22:26:00.470082Z",
     "iopub.status.idle": "2021-06-03T22:26:00.486678Z",
     "shell.execute_reply": "2021-06-03T22:26:00.485915Z",
     "shell.execute_reply.started": "2021-06-03T22:26:00.470553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>bucket_name</th>\n",
       "      <th>stim_version</th>\n",
       "      <th>iterationName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dominoes_pilot</td>\n",
       "      <td>human-physics-benchmarking-dominoes-pilot</td>\n",
       "      <td>production_1</td>\n",
       "      <td>production_1_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collision_pilot</td>\n",
       "      <td>human-physics-benchmarking-collision-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>towers_pilot</td>\n",
       "      <td>human-physics-benchmarking-towers-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linking_pilot</td>\n",
       "      <td>human-physics-benchmarking-linking-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>containment_pilot</td>\n",
       "      <td>human-physics-benchmarking-containment-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rollingsliding_pilot</td>\n",
       "      <td>human-physics-benchmarking-rollingsliding-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drop_pilot</td>\n",
       "      <td>human-physics-benchmarking-drop-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clothiness_pilot</td>\n",
       "      <td>human-physics-benchmarking-clothiness-pilot</td>\n",
       "      <td>production_2</td>\n",
       "      <td>production_2_testing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  study                                      bucket_name  \\\n",
       "0        dominoes_pilot        human-physics-benchmarking-dominoes-pilot   \n",
       "1       collision_pilot       human-physics-benchmarking-collision-pilot   \n",
       "2          towers_pilot          human-physics-benchmarking-towers-pilot   \n",
       "3         linking_pilot         human-physics-benchmarking-linking-pilot   \n",
       "4     containment_pilot     human-physics-benchmarking-containment-pilot   \n",
       "5  rollingsliding_pilot  human-physics-benchmarking-rollingsliding-pilot   \n",
       "6            drop_pilot            human-physics-benchmarking-drop-pilot   \n",
       "7      clothiness_pilot      human-physics-benchmarking-clothiness-pilot   \n",
       "\n",
       "   stim_version         iterationName  \n",
       "0  production_1  production_1_testing  \n",
       "1  production_2  production_2_testing  \n",
       "2  production_2  production_2_testing  \n",
       "3  production_2  production_2_testing  \n",
       "4  production_2  production_2_testing  \n",
       "5  production_2  production_2_testing  \n",
       "6  production_2  production_2_testing  \n",
       "7  production_2  production_2_testing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_meta import *\n",
    "HEM = pd.DataFrame(NEURIPS2021_EXPS) # HEM = \"human experiment metadata\"\n",
    "HEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:00.488405Z",
     "iopub.status.busy": "2021-06-03T22:26:00.487990Z",
     "iopub.status.idle": "2021-06-03T22:26:00.493437Z",
     "shell.execute_reply": "2021-06-03T22:26:00.492314Z",
     "shell.execute_reply.started": "2021-06-03T22:26:00.488369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCENARIOS = sorted([n.split(\"_\")[0] for n in HEM['study'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:00.495338Z",
     "iopub.status.busy": "2021-06-03T22:26:00.495150Z",
     "iopub.status.idle": "2021-06-03T22:26:00.501543Z",
     "shell.execute_reply": "2021-06-03T22:26:00.500554Z",
     "shell.execute_reply.started": "2021-06-03T22:26:00.495318Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get paths to all human response data\n",
    "data_paths = [os.path.join(csv_dir,'humans',i) for i in os.listdir(os.path.join(csv_dir,'humans'))]\n",
    "resp_paths = [i for i in data_paths if i.split('/')[-1].split('-')[0]=='human_responses']\n",
    "assert len(resp_paths)==8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:00.506552Z",
     "iopub.status.busy": "2021-06-03T22:26:00.506336Z",
     "iopub.status.idle": "2021-06-03T22:26:08.536606Z",
     "shell.execute_reply": "2021-06-03T22:26:08.535938Z",
     "shell.execute_reply.started": "2021-06-03T22:26:00.506533Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 104850 lines\n"
     ]
    }
   ],
   "source": [
    "## also load all human data into a big dataframe\n",
    "HD = pd.concat([h.apply_exclusion_criteria(h.load_and_preprocess_data(p)) for p in resp_paths])\n",
    "print(\"Loaded {} lines\".format(len(HD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:08.539900Z",
     "iopub.status.busy": "2021-06-03T22:26:08.539639Z",
     "iopub.status.idle": "2021-06-03T22:26:08.544473Z",
     "shell.execute_reply": "2021-06-03T22:26:08.543451Z",
     "shell.execute_reply.started": "2021-06-03T22:26:08.539864Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get paths to all model data\n",
    "model_data_paths = [os.path.join(csv_dir,'models',i) for i in os.listdir(os.path.join(csv_dir,'models'))]\n",
    "model_res_paths = [i for i in model_data_paths if i.split('.')[-1] == \"csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:08.546629Z",
     "iopub.status.busy": "2021-06-03T22:26:08.546196Z",
     "iopub.status.idle": "2021-06-03T22:26:09.209546Z",
     "shell.execute_reply": "2021-06-03T22:26:09.208690Z",
     "shell.execute_reply.started": "2021-06-03T22:26:08.546604Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 177990 rows\n"
     ]
    }
   ],
   "source": [
    "## load all model results into a single dataframe\n",
    "MD = pd.concat([pd.read_csv(p).assign(filename=p.split('/')[-1]) for p in model_res_paths])\n",
    "print(\"Loaded {} rows\".format(len(MD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:09.210944Z",
     "iopub.status.busy": "2021-06-03T22:26:09.210668Z",
     "iopub.status.idle": "2021-06-03T22:26:11.402474Z",
     "shell.execute_reply": "2021-06-03T22:26:11.401736Z",
     "shell.execute_reply.started": "2021-06-03T22:26:09.210921Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a couple of import steps (restore original scenario names, add single prediction value, add correctness column)\n",
    "MD = h.process_model_dataframe(MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:11.408497Z",
     "iopub.status.busy": "2021-06-03T22:26:11.408216Z",
     "iopub.status.idle": "2021-06-03T22:26:12.268649Z",
     "shell.execute_reply": "2021-06-03T22:26:12.267853Z",
     "shell.execute_reply.started": "2021-06-03T22:26:11.408460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️There are 0 duplicated rows!\n",
      "Removed duplicates, 177990 rows left\n"
     ]
    }
   ],
   "source": [
    "#check for duplicated rows\n",
    "if len(MD.duplicated()) > 0:\n",
    "    print(\"⚠️There are {} duplicated rows!\".format(np.sum(MD.duplicated())))\n",
    "    MD = MD[~MD.duplicated(h.MODEL_COLS+[\"Stimulus Name\"],keep=\"first\")]\n",
    "    print(\"Removed duplicates, {} rows left\".format(len(MD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.270416Z",
     "iopub.status.busy": "2021-06-03T22:26:12.270092Z",
     "iopub.status.idle": "2021-06-03T22:26:12.291323Z",
     "shell.execute_reply": "2021-06-03T22:26:12.290750Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.270354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model kinds to variable\n",
    "MODELS = list(MD[\"Model Kind\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.292949Z",
     "iopub.status.busy": "2021-06-03T22:26:12.292584Z",
     "iopub.status.idle": "2021-06-03T22:26:12.299190Z",
     "shell.execute_reply": "2021-06-03T22:26:12.298286Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.292924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get the following kinds of models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['VGGFrozenMLP_VGG_nan_nan_nan_L2 on latent_0_same',\n",
       " 'RPIN_R-CNN_0.0_L2 on 2D position_all_but_this_L2 on 2D position_0_same',\n",
       " 'RPIN_R-CNN_0.0_L2 on 2D position_all_L2 on 2D position_0_same',\n",
       " 'RPIN_R-CNN_0.0_L2 on 2D position_same_L2 on 2D position_0_same',\n",
       " 'RPIN_R-CNN_1.0_L2 on 2D position_all_but_this_L2 on 2D position_1_same',\n",
       " 'RPIN_R-CNN_2.0_L2 on 2D position_all_but_this_L2 on 2D position_2_same',\n",
       " 'RPIN_R-CNN_1.0_L2 on 2D position_all_L2 on 2D position_1_same',\n",
       " 'RPIN_R-CNN_2.0_L2 on 2D position_all_L2 on 2D position_2_same',\n",
       " 'RPIN_R-CNN_1.0_L2 on 2D position_same_L2 on 2D position_1_same',\n",
       " 'RPIN_R-CNN_2.0_L2 on 2D position_same_L2 on 2D position_2_same',\n",
       " 'SVG_VGG_1.0_VAE_all_but_this_VAE_1_same',\n",
       " 'SVG_VGG_1.0_VAE_all_VAE_1_same',\n",
       " 'SVG_VGG_1.0_VAE_same_VAE_1_same',\n",
       " 'VGGFrozenLSTM_VGG_nan_nan_nan_L2 on latent_0_same',\n",
       " 'CSWM_CSWM encoder_0.0_Contrastive_all_but_this_Contrastive_0_same',\n",
       " 'CSWM_CSWM encoder_0.0_Contrastive_all_Contrastive_0_same',\n",
       " 'CSWM_CSWM encoder_0.0_Contrastive_same_Contrastive_0_same',\n",
       " 'DEITFrozenLSTM_DEIT_nan_nan_nan_L2 on latent_0_same',\n",
       " 'DEITFrozenMLP_DEIT_nan_nan_nan_L2 on latent_0_same',\n",
       " 'OP3_OP3 encoder_0.0_Image Reconstruction_all_but_this_Image Reconstruction_0_same',\n",
       " 'OP3_OP3 encoder_0.0_Image Reconstruction_all_Image Reconstruction_0_same',\n",
       " 'OP3_OP3 encoder_0.0_Image Reconstruction_same_Image Reconstruction_0_same']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"We get the following kinds of models:\")\n",
    "display(MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exclude bad stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.301019Z",
     "iopub.status.busy": "2021-06-03T22:26:12.300639Z",
     "iopub.status.idle": "2021-06-03T22:26:12.345453Z",
     "shell.execute_reply": "2021-06-03T22:26:12.344383Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.300984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37 bad stims\n"
     ]
    }
   ],
   "source": [
    "stim_comparision = pd.merge(pd.DataFrame(MD.groupby('Canon Stimulus Name')['Actual Outcome'].first()).reset_index(),pd.DataFrame(HD.groupby('stim_ID')['target_hit_zone_label'].first()).reset_index(),left_on='Canon Stimulus Name',right_on='stim_ID')\n",
    "\n",
    "bad_stims = stim_comparision[stim_comparision['Actual Outcome'] != stim_comparision['target_hit_zone_label']]['Canon Stimulus Name']\n",
    "print(\"There are {} bad stims\".format(len(bad_stims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.346914Z",
     "iopub.status.busy": "2021-06-03T22:26:12.346540Z",
     "iopub.status.idle": "2021-06-03T22:26:12.457733Z",
     "shell.execute_reply": "2021-06-03T22:26:12.457061Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.346844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Exclude bad stims\n",
    "HD = HD[~HD['stim_ID'].isin(bad_stims)]\n",
    "MD = MD[~MD['Canon Stimulus Name'].isin(bad_stims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.459283Z",
     "iopub.status.busy": "2021-06-03T22:26:12.458940Z",
     "iopub.status.idle": "2021-06-03T22:26:12.635888Z",
     "shell.execute_reply": "2021-06-03T22:26:12.635224Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.459240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Also exclude stims from the rollingsliding ledge subset\n",
    "HD = HD[~HD['stim_ID'].str.contains(\"rollingSliding_simple_ledge\")]\n",
    "MD = MD[~MD['Canon Stimulus Name'].str.contains(\"rollingSliding_simple_ledge\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.637696Z",
     "iopub.status.busy": "2021-06-03T22:26:12.637264Z",
     "iopub.status.idle": "2021-06-03T22:26:12.701722Z",
     "shell.execute_reply": "2021-06-03T22:26:12.700986Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.637657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate labels for regression analysis\n",
    "* Comparison 1: Visual encoder architecture (ConvNet [SVG/VGGFrozenLSTM] vs. transformer [DEITFrozenLSTM] … DEITFrozenMLP vs. SVG/VGGFrozenMLP)\n",
    "* Comparison 2: Dynamics model RNN vs. MLP (LSTM vs. MLP for above)\n",
    "* Comparison 3: Among unsupervised models, object-centric vs. non-object-centric\n",
    "        * {CSWM, OP3} vs. {SVG}\n",
    "* Comparison 4: Latent vs. pixel reconstruction loss\n",
    "        * CSWM vs. OP3\n",
    "* Comparison 5: RPIN vs. CSWM/OP3 (“supervised explicit object-centric” vs. “unsupervised implicit object-centric”)\n",
    "\n",
    "Dimensions: \n",
    "* “Visual encoder architecture” : [“ConvNet” “Transformer” “Neither”]\n",
    "* “Dynamics model architecture” : [“LSTM”, “MLP”, “Neither”]\n",
    "* “ObjectCentric”: [TRUE, FALSE, NA]\n",
    "* “Supervised”: [TRUE, FALSE]\n",
    "* “SelfSupervisedLoss”: [“latent”, “pixel”, “NA”]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:12.703369Z",
     "iopub.status.busy": "2021-06-03T22:26:12.703077Z",
     "iopub.status.idle": "2021-06-03T22:26:13.028588Z",
     "shell.execute_reply": "2021-06-03T22:26:13.027719Z",
     "shell.execute_reply.started": "2021-06-03T22:26:12.703331Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#“Visual encoder architecture” : [“ConvNet” “Transformer” “Neither”]\n",
    "MD['Visual encoder architecture'] = \"Neither\"\n",
    "MD.loc[(MD['Model'].str.contains(\"SVG\")) | (MD['Model'].str.contains(\"VGG\")),'Visual encoder architecture'] = \"ConvNet\"\n",
    "MD.loc[(MD['Model'].str.contains(\"DEIT\")) | (MD['Model'].str.contains(\"VGG\")),'Visual encoder architecture'] = \"Transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:13.031153Z",
     "iopub.status.busy": "2021-06-03T22:26:13.030341Z",
     "iopub.status.idle": "2021-06-03T22:26:13.182584Z",
     "shell.execute_reply": "2021-06-03T22:26:13.181972Z",
     "shell.execute_reply.started": "2021-06-03T22:26:13.030998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# “Dynamics model architecture” : [“LSTM”, “MLP”, “Neither”]\n",
    "MD['Dynamics model architecture'] = \"Neither\"\n",
    "MD.loc[(MD['Model'].str.contains(\"LSTM\")),'Dynamics model architecture'] = \"LSTM\"\n",
    "MD.loc[(MD['Model'].str.contains(\"MLP\")),'Dynamics model architecture'] = \"MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:13.183956Z",
     "iopub.status.busy": "2021-06-03T22:26:13.183633Z",
     "iopub.status.idle": "2021-06-03T22:26:13.387724Z",
     "shell.execute_reply": "2021-06-03T22:26:13.386757Z",
     "shell.execute_reply.started": "2021-06-03T22:26:13.183922Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ObjectCentric”: [TRUE, FALSE, NA]\n",
    "MD['ObjectCentric'] = np.nan\n",
    "MD.loc[(MD['Model'].str.contains(\"CSWM\")) | (MD['Model'].str.contains(\"OP3\")),'ObjectCentric'] = True\n",
    "MD.loc[(MD['Model'].str.contains(\"SVG\")),'ObjectCentric'] = False\n",
    "# MD['ObjectCentric'] = MD['ObjectCentric'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:13.389026Z",
     "iopub.status.busy": "2021-06-03T22:26:13.388827Z",
     "iopub.status.idle": "2021-06-03T22:26:13.808147Z",
     "shell.execute_reply": "2021-06-03T22:26:13.806996Z",
     "shell.execute_reply.started": "2021-06-03T22:26:13.389002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supervised”: [TRUE, FALSE]\n",
    "MD['Supervised'] = np.nan\n",
    "MD.loc[(MD['Model'].str.contains(\"RPIN\")) | (MD['Model'].str.contains(\"DPI\")),'Supervised'] = True\n",
    "MD.loc[(MD['Model'].str.contains(\"CSWM\")) | (MD['Model'].str.contains(\"OP3\")) | (MD['Model'].str.contains(\"SVG\") | (MD['Model'].str.contains(\"VGG\"))),'Supervised'] = False\n",
    "# MD['Supervised'] = MD['Supervised'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:13.810199Z",
     "iopub.status.busy": "2021-06-03T22:26:13.809724Z",
     "iopub.status.idle": "2021-06-03T22:26:14.141538Z",
     "shell.execute_reply": "2021-06-03T22:26:14.140849Z",
     "shell.execute_reply.started": "2021-06-03T22:26:13.810159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SelfSupervisedLoss”: [“latent”, “pixel”, “NA”]\n",
    "MD['SelfSupervisedLossSelfSupervisedLoss'] = \"NA\"\n",
    "MD.loc[(MD['Model'].str.contains(\"CSWM\")),'SelfSupervisedLoss'] = \"latent\"\n",
    "MD.loc[(MD['Model'].str.contains(\"OP3\")) | (MD['Model'].str.contains(\"VGG\")) | (MD['Model'].str.contains(\"SVG\") | (MD['Model'].str.contains(\"VGG\"))),'SelfSupervisedLoss'] = \"pixel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:14.142783Z",
     "iopub.status.busy": "2021-06-03T22:26:14.142532Z",
     "iopub.status.idle": "2021-06-03T22:26:14.146548Z",
     "shell.execute_reply": "2021-06-03T22:26:14.145682Z",
     "shell.execute_reply.started": "2021-06-03T22:26:14.142757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save as model identifying column\n",
    "MODEL_COLS = h.MODEL_COLS + ['Visual encoder architecture','Dynamics model architecture','ObjectCentric','Supervised','SelfSupervisedLossSelfSupervisedLoss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save out \n",
    "MD.to_csv(os.path.join(csv_dir, 'models', 'allModels_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate summary table of human 95% CIs for accuracy across all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:14.148385Z",
     "iopub.status.busy": "2021-06-03T22:26:14.148173Z",
     "iopub.status.idle": "2021-06-03T22:26:25.273123Z",
     "shell.execute_reply": "2021-06-03T22:26:25.272112Z",
     "shell.execute_reply.started": "2021-06-03T22:26:14.148360Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to file! Done.\n"
     ]
    }
   ],
   "source": [
    "## init human_bootstrapped_accuracy for plotting\n",
    "human_bootstrapped_accuracy = pd.DataFrame()\n",
    "\n",
    "for exp_ind, exp_name in enumerate(resp_paths):\n",
    "    \n",
    "    ## get path to response data\n",
    "    path_to_data = resp_paths[exp_ind]\n",
    "\n",
    "    ## load data and apply preprocessing\n",
    "    _D = h.load_and_preprocess_data(path_to_data)\n",
    "    scenarioName = _D.scenarioName.values[0]\n",
    "    print('Currently analyzing the {} experiment.'.format(_D.scenarioName.values[0]))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    ## apply exclusion criteria\n",
    "    D = h.apply_exclusion_criteria(_D)\n",
    "\n",
    "    ## compute bootstrapped sampling distributions of accuracy\n",
    "    Dacc = D.groupby('prolificIDAnon').agg({'correct':np.mean})\n",
    "    bootmeans = h.bootstrap_mean(Dacc, col='correct', nIter=1000)\n",
    "\n",
    "    obsmean = np.mean(Dacc.correct.values)\n",
    "    bootmean = np.mean(bootmeans)\n",
    "    lb = np.percentile(bootmeans,2.5)\n",
    "    ub = np.percentile(bootmeans,97.5)\n",
    "    pct25 = np.percentile(Dacc,2.5)\n",
    "    pct975 = np.percentile(Dacc,97.5)\n",
    "    ## merge bootstrapped accuracy estimates\n",
    "    if len(human_bootstrapped_accuracy)==0:\n",
    "        human_bootstrapped_accuracy = pd.DataFrame(['human', scenarioName, obsmean,bootmean,lb,ub, pct25, pct975]).transpose()\n",
    "    else:\n",
    "        human_bootstrapped_accuracy = pd.concat([human_bootstrapped_accuracy, pd.DataFrame(['human', scenarioName, obsmean,bootmean,lb,ub, pct25, pct975]).transpose()],axis=0)\n",
    "        \n",
    "## add column names        \n",
    "human_bootstrapped_accuracy.columns=['agent','scenario','obs_mean', 'boot_mean', 'ci_lb', 'ci_ub', 'pct_2.5', 'pct_97.5']\n",
    "\n",
    "## save out human_bootstrapped_accuracy to re-plot in R\n",
    "if not os.path.exists(os.path.join(csv_dir, 'summary')):\n",
    "    os.makedirs(os.path.join(csv_dir, 'summary'))    \n",
    "human_bootstrapped_accuracy.to_csv(os.path.join(csv_dir, 'summary','human_accuracy_by_scenario.csv'), index=False)\n",
    "print('Saved to file! Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:25.282646Z",
     "iopub.status.busy": "2021-06-03T22:26:25.282417Z",
     "iopub.status.idle": "2021-06-03T22:26:25.295412Z",
     "shell.execute_reply": "2021-06-03T22:26:25.294819Z",
     "shell.execute_reply.started": "2021-06-03T22:26:25.282624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_bootstrapped_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-human consistency across stimuli (within scenario)\n",
    "We will analyze human-human consistency by computing the mean correlation between (binary) response vectors produced by each human participant across all stimuli within each scenario. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:25.299368Z",
     "iopub.status.busy": "2021-06-03T22:26:25.298931Z",
     "iopub.status.idle": "2021-06-03T22:26:32.969724Z",
     "shell.execute_reply": "2021-06-03T22:26:32.969038Z",
     "shell.execute_reply.started": "2021-06-03T22:26:25.299330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## init human_boot_corr for plotting\n",
    "human_boot_corr = pd.DataFrame()\n",
    "\n",
    "for exp_ind, exp_name in enumerate(resp_paths):\n",
    "    \n",
    "    ## get path to response data\n",
    "    path_to_data = resp_paths[exp_ind]\n",
    "\n",
    "    ## load data and apply preprocessing\n",
    "    _D = h.load_and_preprocess_data(path_to_data)\n",
    "    scenarioName = _D.scenarioName.values[0]\n",
    "    print('Currently analyzing the {} experiment.'.format(_D.scenarioName.values[0]))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## apply exclusion criteria\n",
    "    D = h.apply_exclusion_criteria(_D)\n",
    "    \n",
    "    ## create response feature matrix (numSubs x numTrialsPerSub)\n",
    "    D2 = D.sort_values(by=['prolificIDAnon','stim_ID']).reset_index(drop=True)\n",
    "    numSubs = len(np.unique(D['prolificIDAnon'].values))\n",
    "    numTrialsPerSub = int(len(D)/numSubs)\n",
    "    respMat = np.reshape(D2['responseBool'].values, (numSubs,numTrialsPerSub)) \n",
    "\n",
    "    ## sanity check that the reshape operation happened correctly\n",
    "    assert len([i for (i,j) in list(zip(respMat[0],D2[:150]['responseBool'].values)) if i!=j])==0    \n",
    "    \n",
    "    ## get pairwise correlations\n",
    "    dists = 1-scipy.spatial.distance.pdist(respMat, metric='correlation')\n",
    "    corrMat = scipy.spatial.distance.squareform(dists)\n",
    "    \n",
    "    ## get percentiles over pairwise corrs\n",
    "    pairwiseCorrs = corrMat[np.triu_indices(n=len(corrMat), k=1)]\n",
    "    lb = np.percentile(pairwiseCorrs, 2.5)\n",
    "    med = np.percentile(pairwiseCorrs, 50)\n",
    "    ub = np.percentile(pairwiseCorrs, 97.5)  \n",
    "        \n",
    "    if len(human_boot_corr)==0:\n",
    "        human_boot_corr = pd.DataFrame(['human', scenarioName, lb, med, ub]).transpose()\n",
    "    else:\n",
    "        human_boot_corr = pd.concat([human_boot_corr, pd.DataFrame(['human', scenarioName, lb, med, ub]).transpose()],axis=0)\n",
    "        \n",
    "## add column names        \n",
    "human_boot_corr.columns=['agent','scenario','corr_lb', 'corr_med', 'corr_ub']\n",
    "\n",
    "## save out human_boot_corr to re-plot in R\n",
    "if not os.path.exists(os.path.join(csv_dir, 'summary')):\n",
    "    os.makedirs(os.path.join(csv_dir, 'summary'))    \n",
    "human_boot_corr.to_csv(os.path.join(csv_dir, 'summary','human_pairwiseCorrs_by_scenario.csv'), index=False)\n",
    "print('Saved to file! Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:32.971856Z",
     "iopub.status.busy": "2021-06-03T22:26:32.971361Z",
     "iopub.status.idle": "2021-06-03T22:26:32.985143Z",
     "shell.execute_reply": "2021-06-03T22:26:32.983785Z",
     "shell.execute_reply.started": "2021-06-03T22:26:32.971810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_boot_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen's $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:32.987150Z",
     "iopub.status.busy": "2021-06-03T22:26:32.986781Z",
     "iopub.status.idle": "2021-06-03T22:26:55.625918Z",
     "shell.execute_reply": "2021-06-03T22:26:55.625098Z",
     "shell.execute_reply.started": "2021-06-03T22:26:32.987079Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## init human_boot_cohenk for plotting\n",
    "human_boot_cohenk = pd.DataFrame()\n",
    "\n",
    "for exp_ind, exp_name in enumerate(resp_paths):\n",
    "    \n",
    "    ## get path to response data\n",
    "    path_to_data = resp_paths[exp_ind]\n",
    "\n",
    "    ## load data and apply preprocessing\n",
    "    _D = h.load_and_preprocess_data(path_to_data)\n",
    "    scenarioName = _D.scenarioName.values[0]\n",
    "    print('Currently analyzing the {} experiment.'.format(_D.scenarioName.values[0]))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## apply exclusion criteria\n",
    "    D = h.apply_exclusion_criteria(_D)\n",
    "    \n",
    "    ## create response feature matrix (numSubs x numTrialsPerSub)\n",
    "    D2 = D.sort_values(by=['prolificIDAnon','stim_ID']).reset_index(drop=True)\n",
    "    numSubs = len(np.unique(D['prolificIDAnon'].values))\n",
    "    numTrialsPerSub = int(len(D)/numSubs)\n",
    "    respMat = np.reshape(D2['responseBool'].values, (numSubs,numTrialsPerSub)) \n",
    "\n",
    "    ## sanity check that the reshape operation happened correctly\n",
    "    assert len([i for (i,j) in list(zip(respMat[0],D2[:150]['responseBool'].values)) if i!=j])==0    \n",
    "      \n",
    "    ## compute Cohen's kappa\n",
    "    ## with a horrific double loop\n",
    "    kappas = []\n",
    "    for i in range(respMat.shape[0]): # for each participant\n",
    "        for j in range(i+1,respMat.shape[0]): # compare to every participant after them\n",
    "            assert i != j\n",
    "            kappa = sklearn.metrics.cohen_kappa_score(respMat[i],respMat[j])\n",
    "            kappas.append(kappa)\n",
    "    \n",
    "    ## get percentiles over pairwise corrs\n",
    "    lb = np.percentile(kappas, 2.5)\n",
    "    med = np.percentile(kappas, 50)\n",
    "    ub = np.percentile(kappas, 97.5)  \n",
    "        \n",
    "    if len(human_boot_cohenk)==0:\n",
    "        human_boot_cohenk = pd.DataFrame(['human', scenarioName, lb, med, ub]).transpose()\n",
    "    else:\n",
    "        human_boot_cohenk = pd.concat([human_boot_cohenk, pd.DataFrame(['human', scenarioName, lb, med, ub]).transpose()],axis=0)\n",
    "        \n",
    "## add column names        \n",
    "human_boot_cohenk.columns=['agent','scenario','corr_lb', 'corr_med', 'corr_ub']\n",
    "\n",
    "## save out human_boot_cohenk to re-plot in R\n",
    "if not os.path.exists(os.path.join(csv_dir, 'summary')):\n",
    "    os.makedirs(os.path.join(csv_dir, 'summary'))    \n",
    "human_boot_cohenk.to_csv(os.path.join(csv_dir, 'summary','human_pairwiseCohensKs_by_scenario.csv'), index=False)\n",
    "print('Saved to file! Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:55.627906Z",
     "iopub.status.busy": "2021-06-03T22:26:55.627663Z",
     "iopub.status.idle": "2021-06-03T22:26:55.642502Z",
     "shell.execute_reply": "2021-06-03T22:26:55.641626Z",
     "shell.execute_reply.started": "2021-06-03T22:26:55.627880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_boot_cohenk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution of model physical judgments, by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute summary statistics over model physical judgments, by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:55.644595Z",
     "iopub.status.busy": "2021-06-03T22:26:55.644129Z",
     "iopub.status.idle": "2021-06-03T22:26:56.004160Z",
     "shell.execute_reply": "2021-06-03T22:26:56.003351Z",
     "shell.execute_reply.started": "2021-06-03T22:26:55.644555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Dacc = D.groupby('stim_ID').agg({'correct':np.mean})\n",
    "p = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct human-model comparisons\n",
    "We will compare human and model behavior in two ways: **absolute performance** and **response pattern.**\n",
    "\n",
    "#### **Absolute Performance** \n",
    "We will compare the accuracy of each model to the mean accuracy of humans, for each scenario. \n",
    "To do this, we will first compute estimates of mean human accuracy for each scenario and construct 95% confidence intervals for each of these estimates. \n",
    "These confidence intervals will be constructed by bootstrapping: specifically, for an experiment with N participants, we will resample N participants with replacement and compute the proportion correct for that bootstrapped sample. We will take repeat this resampling procedure 1000 times to generate a sampling distribution for the mean proportion correct. The 2.5th and 97.5th percentile will be extracted from this sampling distribution to provide the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "For each model, we will then compare their proportion correct (a point estimate) to the human confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:56.005598Z",
     "iopub.status.busy": "2021-06-03T22:26:56.005314Z",
     "iopub.status.idle": "2021-06-03T22:26:56.199393Z",
     "shell.execute_reply": "2021-06-03T22:26:56.198699Z",
     "shell.execute_reply.started": "2021-06-03T22:26:56.005571Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group model data by scenario\n",
    "MD_by_scenario = MD.groupby(['Readout Test Data','ModelID']).agg(\n",
    "        {**{ 'correct':'mean' },\n",
    "         **{ col:'first' for col in MODEL_COLS+h.DATASET_ABSTRACTED_COLS} #save model identifying data as well\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:56.200807Z",
     "iopub.status.busy": "2021-06-03T22:26:56.200631Z",
     "iopub.status.idle": "2021-06-03T22:26:57.954792Z",
     "shell.execute_reply": "2021-06-03T22:26:57.953906Z",
     "shell.execute_reply.started": "2021-06-03T22:26:56.200787Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to file. Done!\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for scenario in sorted(MD['Readout Test Data'].unique()):\n",
    "    print(\"Now running scenario\",scenario)\n",
    "    _MD_by_scenario = MD_by_scenario.loc[[scenario]]\n",
    "    for _,model_row in list(_MD_by_scenario.iterrows()):\n",
    "        #each model is one row of MD\n",
    "        human_row = human_bootstrapped_accuracy.query(\"scenario == @scenario\")\n",
    "#         assert len(model_row) == len(human_row) == 1\n",
    "        correct_ratio = model_row['correct']/human_row['obs_mean']\n",
    "        correct_diff = model_row['correct'] - human_row['obs_mean']\n",
    "        accuracies[(scenario,model_row.name[1])] = {**{\n",
    "                                                    'scenario': scenario,\n",
    "                                                    'ratio': float(correct_ratio), \n",
    "                                                    'diff': float(correct_diff),\n",
    "                                                    'human_correct': float(human_row['obs_mean']),\n",
    "                                                    'model_correct': float(model_row['correct']),\n",
    "                                                    },**{col: model_row[col] for col in MODEL_COLS+h.DATASET_ABSTRACTED_COLS}} # save information for model identification\n",
    "    clear_output(wait=True)\n",
    "\n",
    "model_human_accuracies = pd.DataFrame(accuracies).transpose()  \n",
    "model_human_accuracies.to_csv(os.path.join(csv_dir, 'summary','model_human_accuracies.csv'), index=False)\n",
    "print('Saved to file. Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:26:57.956809Z",
     "iopub.status.busy": "2021-06-03T22:26:57.956203Z",
     "iopub.status.idle": "2021-06-03T22:26:57.991023Z",
     "shell.execute_reply": "2021-06-03T22:26:57.990131Z",
     "shell.execute_reply.started": "2021-06-03T22:26:57.956776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Response Pattern**\n",
    "We will compare the pattern of predictions generated by each model to the pattern of predictions generated by humans. \n",
    "\n",
    "We will do this by using two standard inter-rater reliability metrics:\n",
    "\n",
    "##### **Correlation between average-human and model responses**\n",
    "For each stimulus, we will compute the proportion of \"hit\" responses by humans. \n",
    "For each stimulus, we will extract the hit probability generated by models.\n",
    "For each scenario (i.e., domain), we will compute the root-mean-squared-deviation between the human proportion-hit vector and the model probability-hit vector. \n",
    "To estimate variability across human samples, we will conduct bootstrap resampling (i.e., resampling data from individual participants with replacement), where for each bootstrap sample we will re-compute the correlation between the model probability-hit vector and the (bootstrapped) human proportion-hit vector.\n",
    "\n",
    "##### **Correlation** (DEPRECATED, SUPERSEDED BY COHEN's KAPPA BELOW, WHICH CORRECTS FOR CHANCE AGREEMENT RATE)\n",
    "For each pair of human participants, we will compute the correlation between their (binary) response vectors, yielding a distribution of pairwise human-human correlations. \n",
    "For each model, we will compute the correlation between its response vector and every human participant, as well as every other model. \n",
    "A model's response pattern will be considered more similar to humans' insofar as the mean model-human correlation (across humans) lies closer to the mean human-human correlation (for all pairs of humans).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "\n",
    "\n",
    "for scenario in sorted(MD['Readout Test Data'].unique()):\n",
    "    print(\"Now running scenario\",scenario)\n",
    "    _MD = MD[MD['Readout Test Data'] == scenario]\n",
    "    _HD = HD[HD['scenarioName'] == scenario].sort_values('stim_ID')\n",
    "    for model in _MD['ModelID'].unique():\n",
    "        #get responses of model        \n",
    "        _MD_model = _MD[_MD['ModelID'] == model]\n",
    "        _MD_model = _MD_model.sort_values('Canon Stimulus Name') #ensure same stim order \n",
    "        \n",
    "        ## get average human response vector\n",
    "        _HD_resp = _HD.groupby('stim_ID')['responseBool'].mean().reset_index()\n",
    "        #in case the models have more or less responses compared to humans\n",
    "        human_stim_names = set(list(_HD['stim_ID']))\n",
    "        model_stim_names = set(list(_MD_model['Canon Stimulus Name']))\n",
    "        joint_stim_names = human_stim_names.intersection(model_stim_names)\n",
    "        if len(joint_stim_names) == 0:\n",
    "            print(\"⛔️ {} is missing all datapoints on {} human responses\".format(model, len(human_stim_names)),end=\"\\r\")\n",
    "            continue #ignore and move on\n",
    "        if len(human_stim_names) > len(joint_stim_names):\n",
    "            print(\"⚠️ {} is missing {} datapoints on {} human responses\".format(model,len(human_stim_names) - len(joint_stim_names), len(human_stim_names)),end=\"\\r\")\n",
    "\n",
    "        #subset both models to ensure only common stims are used\n",
    "        _MD_model = _MD_model[_MD_model['Canon Stimulus Name'].isin(joint_stim_names)]            \n",
    "        _HD_resp = _HD_resp[_HD_resp['stim_ID'].isin(joint_stim_names)]           \n",
    "        ## make sure order is exactly the same\n",
    "        assert len([i for (i,j) in zip(_MD_model['Canon Stimulus Name'].values, _HD_resp['stim_ID'].values) if i!=j])==0\n",
    "        \n",
    "        ## extract human & model responses as arrays\n",
    "        model_responses = _MD_model['Predicted Prob_true'].values\n",
    "        human_responses = _HD_resp['responseBool'].values\n",
    "\n",
    "        ## compute RMSE per stimulus\n",
    "        RMSE = scipy.spatial.distance.euclidean(model_responses, human_responses) / len(model_responses)\n",
    "        \n",
    "        out_dict[(scenario, model)] = {**{'scenario':scenario,\n",
    "                                          'modelID': model,\n",
    "                                          'RMSE':RMSE,\n",
    "                                          'num_datapoints':len(model_responses)},\n",
    "                                           **{col:_MD_model.head(1)[col].item() for col in MODEL_COLS+h.DATASET_ABSTRACTED_COLS} #save model ID info\n",
    "                                      }\n",
    "        clear_output(wait=True)        \n",
    "\n",
    "model_human_rmse = pd.DataFrame(out_dict).transpose()  \n",
    "model_human_rmse.columns = model_human_rmse.columns.get_level_values(0) ## flatten multi-level index\n",
    "model_human_rmse.reset_index(drop=True) ## get rid of multi-level index\n",
    "model_human_rmse = model_human_rmse.assign(RMSE = pd.to_numeric(model_human_rmse['RMSE']))\n",
    "model_human_rmse.to_csv(os.path.join(csv_dir, 'summary','model_human_rmse.csv'), index=False)\n",
    "print('Saved to file. Done!')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 1000\n",
    "idx = np.random.randint(0,_HD.gameID.nunique(),size=(1000,_HD.gameID.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameIDs = np.unique(_HD.gameID.values)\n",
    "gameIDs[idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precompute indices for bootstrap resampling\n",
    "nIter = 1000\n",
    "idx = np.random.RandomState(123).randint(0,_HD.gameID.nunique(),size=(1000,_HD.gameID.nunique()))\n",
    "\n",
    "currIter = 0\n",
    "boot_games = gameIDs[idx[currIter]]\n",
    "boot_data = []\n",
    "for game in boot_games:\n",
    "    boot_data.append(_HD[_HD['gameID']==game])\n",
    "boot_data = pd.concat(boot_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct bootstrapping on model-avgHuman distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "nIter = 100  ## num bootstrap iterations\n",
    "out_dict = {} ## initialize results dataframe\n",
    "\n",
    "for scenario_ind, scenario in enumerate(sorted(MD['Readout Test Data'].unique())):\n",
    "    \n",
    "    _MD = MD[MD['Readout Test Data'] == scenario]\n",
    "    _HD = HD[HD['scenarioName'] == scenario].sort_values('stim_ID')\n",
    "    \n",
    "    ## precompute indices for bootstrap resampling\n",
    "    gameIDs = np.unique(_HD.gameID.values)\n",
    "    idx = np.random.RandomState(123).randint(0,_HD.gameID.nunique(),size=(1000,_HD.gameID.nunique()))\n",
    "\n",
    "    for model_ind, model in enumerate(_MD['ModelID'].unique()):\n",
    "        #get responses of model        \n",
    "        _MD_model = _MD[_MD['ModelID'] == model]\n",
    "        _MD_model = _MD_model.sort_values('Canon Stimulus Name') #ensure same stim order \n",
    "        \n",
    "        for currIter in np.arange(nIter):\n",
    "            boot_games = gameIDs[idx[currIter]]\n",
    "            _HD_boot = []\n",
    "            for game in boot_games:\n",
    "                _HD_boot.append(_HD[_HD['gameID']==game])\n",
    "            _HD_boot = pd.concat(_HD_boot)            \n",
    "\n",
    "            ## get average human response vector (bootstrapped)\n",
    "            _HD_resp = _HD_boot.groupby('stim_ID')['responseBool'].mean().reset_index()\n",
    "            #in case the models have more or less responses compared to humans\n",
    "            human_stim_names = set(list(_HD['stim_ID']))\n",
    "            model_stim_names = set(list(_MD_model['Canon Stimulus Name']))\n",
    "            joint_stim_names = human_stim_names.intersection(model_stim_names)\n",
    "            if len(joint_stim_names) == 0:\n",
    "                print(\"⛔️ {} is missing all datapoints on {} human responses\".format(model, len(human_stim_names)),end=\"\\r\")\n",
    "                continue #ignore and move on\n",
    "            if len(human_stim_names) > len(joint_stim_names):\n",
    "                print(\"⚠️ {} is missing {} datapoints on {} human responses\".format(model,len(human_stim_names) - len(joint_stim_names), len(human_stim_names)),end=\"\\r\")\n",
    "\n",
    "            #subset both models to ensure only common stims are used\n",
    "            _MD_model = _MD_model[_MD_model['Canon Stimulus Name'].isin(joint_stim_names)]            \n",
    "            _HD_resp = _HD_resp[_HD_resp['stim_ID'].isin(joint_stim_names)]           \n",
    "            ## make sure order is exactly the same\n",
    "            assert len([i for (i,j) in zip(_MD_model['Canon Stimulus Name'].values, _HD_resp['stim_ID'].values) if i!=j])==0\n",
    "\n",
    "            ## extract human & model responses as arrays\n",
    "            model_responses = _MD_model['Predicted Prob_true'].values\n",
    "            human_responses = _HD_resp['responseBool'].values\n",
    "\n",
    "            ## compute RMSE per stimulus\n",
    "            RMSE = scipy.spatial.distance.euclidean(model_responses, human_responses) / len(model_responses)\n",
    "\n",
    "            out_dict[(scenario, model, currIter)] = {**{'scenario':scenario,\n",
    "                                                      'modelID': model,\n",
    "                                                      'bootstrap_ind': currIter,\n",
    "                                                      'RMSE':RMSE,\n",
    "                                                      'num_datapoints':len(model_responses)},\n",
    "                                                       **{col:_MD_model.head(1)[col].item() for col in MODEL_COLS+h.DATASET_ABSTRACTED_COLS} #save model ID info\n",
    "                                                  }\n",
    "            elapsed_time = np.round(time.time() - start_time,1)\n",
    "            print(\"Now running: scenario {} {} | model {} {}| iteration {} | elapsed time {} seconds\".format(scenario_ind, scenario, model_ind, model, currIter,elapsed_time))\n",
    "            clear_output(wait=True)        \n",
    "\n",
    "model_human_rmse_boot = pd.DataFrame(out_dict).transpose()  \n",
    "model_human_rmse_boot.columns = model_human_rmse_boot.columns.get_level_values(0) ## flatten multi-level index\n",
    "model_human_rmse_boot = model_human_rmse_boot.reset_index(drop=True) ## get rid of multi-level index\n",
    "model_human_rmse_boot = model_human_rmse_boot.assign(RMSE = pd.to_numeric(model_human_rmse['RMSE']))\n",
    "model_human_rmse_boot.to_csv(os.path.join(csv_dir, 'summary','model_human_rmse_bootstrapped.csv'), index=False)\n",
    "print('Saved to file. Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cohen's kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:30:51.034991Z",
     "iopub.status.busy": "2021-06-03T22:30:51.034463Z",
     "iopub.status.idle": "2021-06-03T22:35:20.911226Z",
     "shell.execute_reply": "2021-06-03T22:35:20.909848Z",
     "shell.execute_reply.started": "2021-06-03T22:30:51.034953Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "out_dict = {}\n",
    "\n",
    "for scenario in sorted(MD['Readout Test Data'].unique()):\n",
    "    _MD = MD[MD['Readout Test Data'] == scenario]\n",
    "    _HD = HD[HD['scenarioName'] == scenario]\n",
    "    for model in _MD['ModelID'].unique():\n",
    "        measures_for_model = []\n",
    "        #get responses of model        \n",
    "        _MD_model = _MD[_MD['ModelID'] == model]\n",
    "        _MD_model = _MD_model.sort_values('Canon Stimulus Name') #ensure same stim order\n",
    "        #iterate over the 100 or so participants\n",
    "        for gameID in _HD['gameID'].unique():\n",
    "            #get one game\n",
    "            _HD_game = _HD[_HD['gameID']==gameID]\n",
    "            #ensure stim order\n",
    "            _HD_game = _HD_game.sort_values('stim_ID')\n",
    "            #in case the models have more or less responses compared to humans\n",
    "            human_stim_names = list(_HD_game['stim_ID'])\n",
    "            model_stim_names = list(_MD_model['Canon Stimulus Name'])\n",
    "            joint_stim_names = set(human_stim_names).intersection(set(model_stim_names))\n",
    "            if len(joint_stim_names) == 0:\n",
    "                print(\"⛔️ {} is missing all datapoints on {} human responses\".format(model, len(human_stim_names)),end=\"\\r\")\n",
    "                continue #ignore and move on\n",
    "            if len(human_stim_names) > len(joint_stim_names):\n",
    "                print(\"⚠️ {} is missing {} datapoints on {} human responses\".format(model,len(human_stim_names) - len(joint_stim_names), len(human_stim_names)),end=\"\\r\")\n",
    "            #subset both models to ensure only common stims are used\n",
    "            _MD_model = _MD_model[_MD_model['Canon Stimulus Name'].isin(joint_stim_names)]            \n",
    "            _HD_game = _HD_game[_HD_game['stim_ID'].isin(joint_stim_names)]\n",
    "            #pull response vector\n",
    "            human_responses = np.array(_HD_game['responseBool'].astype(int)) #get human response and cast to int\n",
    "            model_responses = np.array(_MD_model['Predicted Outcome'])\n",
    "#             assert list(model_stim_names) == list(human_stim_names), \"experimental and test stims don't match\"\n",
    "            assert len(model_responses) == len(human_responses), \"More than 1 observation per stimulus\"\n",
    "            # compute Cohen's kappa\n",
    "            measure = sklearn.metrics.cohen_kappa_score(model_responses,human_responses)\n",
    "            measures_for_model.append(measure)\n",
    "        if len(measures_for_model) == 0:\n",
    "            print(\"⛔️ {} is missing all datapoints on human responses\".format(model))\n",
    "            continue\n",
    "        # get percentiles over the range of measures\n",
    "        lb = np.percentile(measures_for_model, 2.5)\n",
    "        med = np.percentile(measures_for_model, 50)\n",
    "        ub = np.percentile(measures_for_model, 97.5)\n",
    "        out_dict[(scenario, model)] = {**{'scenario':scenario,\n",
    "                                       'Cohens_k_lb':lb,\n",
    "                                       'Cohens_k_med':med,\n",
    "                                       'Cohens_k_ub':ub,\n",
    "                                        'num_datapoints':len(measures_for_model)},\n",
    "                                      **{col:_MD_model.head(1)[col].item() for col in MODEL_COLS+h.DATASET_ABSTRACTED_COLS} #save model ID info\n",
    "                                      }\n",
    "    \n",
    "    elapsed_time = np.round(time.time() - start_time,1)\n",
    "    print(\"Now running: scenario {} | model {}| elapsed time {} seconds\".format(scenario, model, elapsed_time))\n",
    "    clear_output(wait=True)        \n",
    "\n",
    "model_human_CohensK = pd.DataFrame(out_dict).transpose()    \n",
    "model_human_CohensK.to_csv(os.path.join(csv_dir, 'summary','model_human_CohensK.csv'), index=False)\n",
    "print('Saved to file. Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:35:20.913624Z",
     "iopub.status.busy": "2021-06-03T22:35:20.913006Z",
     "iopub.status.idle": "2021-06-03T22:35:20.950072Z",
     "shell.execute_reply": "2021-06-03T22:35:20.949173Z",
     "shell.execute_reply.started": "2021-06-03T22:35:20.913581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_human_CohensK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model/human figure\n",
    "Results: Mega bar plot(s)/tables of model accuracies and human response correlations[Eli/Judy/Felix]. Models (~16) x Training (3) x Readout (3) x Target (2) x Scenarios (8) = 144 bars per model. That’s a lot! Needs careful thinking about how to display.\n",
    "\n",
    "Essentially outer y axis is scenario, outer x is measure (accuracy, correlation, Cohens kappa). Inner y for each square is the measure, inner x is models. Filled out dots are full procedure, not-filled out is without dynamics prediction. Humans are a zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Very work in progress==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:35:20.951487Z",
     "iopub.status.busy": "2021-06-03T22:35:20.951235Z",
     "iopub.status.idle": "2021-06-03T22:35:20.989546Z",
     "shell.execute_reply": "2021-06-03T22:35:20.988965Z",
     "shell.execute_reply.started": "2021-06-03T22:35:20.951466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_human_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:35:20.991202Z",
     "iopub.status.busy": "2021-06-03T22:35:20.990820Z",
     "iopub.status.idle": "2021-06-03T22:35:21.003853Z",
     "shell.execute_reply": "2021-06-03T22:35:21.003166Z",
     "shell.execute_reply.started": "2021-06-03T22:35:20.991166Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MD['Dynamics Training Dataset Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:35:21.005799Z",
     "iopub.status.busy": "2021-06-03T22:35:21.005461Z",
     "iopub.status.idle": "2021-06-03T22:35:21.011114Z",
     "shell.execute_reply": "2021-06-03T22:35:21.009729Z",
     "shell.execute_reply.started": "2021-06-03T22:35:21.005776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "READOUTS = [(\"C\",\"full\"),\n",
    "            (\"B\",\"none\")]\n",
    "\n",
    "TRAININGS = [\n",
    "    ('all_but_this',\"^\"),\n",
    "    ('all',\"s\"),\n",
    "    ('same',\"o\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T22:35:21.013253Z",
     "iopub.status.busy": "2021-06-03T22:35:21.012659Z",
     "iopub.status.idle": "2021-06-03T22:35:21.017144Z",
     "shell.execute_reply": "2021-06-03T22:35:21.016137Z",
     "shell.execute_reply.started": "2021-06-03T22:35:21.013213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set up color map over models\n",
    "from matplotlib import cm\n",
    "\n",
    "#cmap = cm.tab20.colors ## not enough colors\n",
    "turbo = cm.get_cmap('turbo', 22)\n",
    "cmap = turbo(np.linspace(0, 1, 22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T00:06:21.450067Z",
     "iopub.status.busy": "2021-06-04T00:06:21.449790Z",
     "iopub.status.idle": "2021-06-04T00:06:36.068268Z",
     "shell.execute_reply": "2021-06-04T00:06:36.067580Z",
     "shell.execute_reply.started": "2021-06-04T00:06:21.450029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(MD['Readout Test Data'].unique()),3, figsize=(40,40), sharex=True, sharey=False)\n",
    "\n",
    "models = sorted(MD['Model Kind'].unique())\n",
    "\n",
    "for outer_y, scenario in enumerate(sorted(MD['Readout Test Data'].unique())):\n",
    "    # ———accuracy plot———\n",
    "    axis = axes[outer_y,0]\n",
    "    axis.set_title(scenario + \" accuracy\")\n",
    "    #plot human zone\n",
    "    human_row = human_bootstrapped_accuracy[human_bootstrapped_accuracy['scenario'] == scenario]\n",
    "    lb = float(human_row['ci_lb'])\n",
    "    ub = float(human_row['ci_ub'])\n",
    "    mean = float(human_row['obs_mean'])\n",
    "    axis.axhspan(lb,ub,color=(.8,.9,1))\n",
    "    axis.set_xticks(np.arange(len(models)))\n",
    "    axis.set_xticklabels(models,rotation=90)\n",
    "    \n",
    "    #plot models\n",
    "    for x,model in enumerate(models):\n",
    "        #get relevant rows\n",
    "        rows = model_human_accuracies[(model_human_accuracies['Model Kind'] == model) & (model_human_accuracies['scenario'] == scenario)]\n",
    "        for readout,fill in READOUTS:\n",
    "            for i,(training,symbol) in enumerate(TRAININGS):\n",
    "                row = rows[(rows['Readout Type']==readout) & (rows['Dynamics Training Dataset Type'] == training)]\n",
    "                if len(row) > 0: #do we have something to plot?\n",
    "                    assert len(row) == 1, \"More than one entry for specific agent\"\n",
    "                    lb = row['model_correct']\n",
    "                    ub = row['model_correct']\n",
    "                    measure = row['model_correct']\n",
    "                    ub = abs(ub - measure)\n",
    "                    lb = abs(lb - measure)\n",
    "                    axis.errorbar(x+(i*.333)+(fill==\"none\")*.166,measure,(lb,ub),marker=symbol,fillstyle=fill,label=model,color=cmap[x])\n",
    "        axis.set_ybound([0,1])\n",
    "    \n",
    "    # ———correlation plot———\n",
    "#     axis = axes[outer_y,1]\n",
    "#     axis.set_title(scenario + \" correlation\")\n",
    "#     #plot human zone\n",
    "#     human_row = human_boot_corr[human_boot_corr['scenario'] == scenario]\n",
    "#     lb = float(human_row['corr_lb'])\n",
    "#     ub = float(human_row['corr_ub'])\n",
    "#     mean = float(human_row['corr_med'])\n",
    "#     axis.axhspan(lb,ub,color=(.8,.9,1))\n",
    "#     axis.set_xticks(np.arange(len(models)))\n",
    "#     axis.set_xticklabels(models,rotation=90)\n",
    "\n",
    "    #plot models\n",
    "#     for x,model in enumerate(models):\n",
    "#         #get relevant rows\n",
    "#         rows = model_human_rmse[(model_human_rmse['Model Kind'] == model) & (model_human_rmse['scenario'] == scenario)]\n",
    "#         for readout,fill in READOUTS:\n",
    "#             for i,(training,symbol) in enumerate(TRAININGS):\n",
    "#                 row = rows[(rows['Readout Type']==readout) & (rows['Dynamics Training Dataset Type'] == training)]\n",
    "#                 if len(row) > 0: #do we have something to plot?\n",
    "#                     assert len(row) == 1, \"More than one entry for specific agent\"\n",
    "#                     lb = row['corr_lb']\n",
    "#                     ub = row['corr_ub']\n",
    "#                     measure = row['corr_med']\n",
    "#                     ub = abs(ub - measure)\n",
    "#                     lb = abs(lb - measure)\n",
    "#                     axis.errorbar(x+(i*.333)+(fill==\"none\")*.166,measure,(lb,ub),marker=symbol,fillstyle=fill,label=model,color=cmap[x])\n",
    "#         axis.set_ybound([0,1])\n",
    "        \n",
    "    \n",
    "    # ———Cohens'k plot———\n",
    "    axis = axes[outer_y,2]\n",
    "    axis.set_title(scenario + \" Cohen's kappa\")\n",
    "    #plot human zone\n",
    "    human_row = human_boot_cohenk[human_boot_cohenk['scenario'] == scenario]\n",
    "    lb = float(human_row['corr_lb'])\n",
    "    ub = float(human_row['corr_ub'])\n",
    "    mean = float(human_row['corr_med'])\n",
    "    axis.axhspan(lb,ub,color=(.8,.9,1))\n",
    "    axis.set_xticks(np.arange(len(models)))\n",
    "    axis.set_xticklabels(models,rotation=90)\n",
    "\n",
    "    #plot models\n",
    "    for x,model in enumerate(models):\n",
    "        #get relevant rows\n",
    "        rows = model_human_CohensK[(model_human_CohensK['Model Kind'] == model) & (model_human_CohensK['scenario'] == scenario)]\n",
    "        for readout,fill in READOUTS:\n",
    "            for i,(training,symbol) in enumerate(TRAININGS):\n",
    "                row = rows[(rows['Readout Type']==readout) & (rows['Dynamics Training Dataset Type'] == training)]\n",
    "                if len(row) > 0: #do we have something to plot?\n",
    "                    assert len(row) == 1, \"More than one entry for specific agent\"\n",
    "                    lb = row['Cohens_k_lb']\n",
    "                    ub = row['Cohens_k_ub']\n",
    "                    measure = row['Cohens_k_med']\n",
    "                    ub = abs(ub - measure)\n",
    "                    lb = abs(lb - measure)\n",
    "                    axis.errorbar(x+(i*.333)+(fill==\"none\")*.166,measure,(lb,ub),marker=symbol,fillstyle=fill,color=cmap[x])\n",
    "        axis.set_ybound([-.25\n",
    "                         ,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_bootstrapped_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
