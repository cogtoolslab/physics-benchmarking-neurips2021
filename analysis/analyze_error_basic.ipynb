{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of human error rate under different experiment parameters across scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to:** \n",
    "* Apply preprocessing to human behavioral data\n",
    "* Visualize distribution and compute summary statistics over **human** physical judgments\n",
    "* Conduct error analysis on which scenarios/ instances did humans make lots of error\n",
    "* Conduct more detailed error analysis on which scenarios/ instances did humans and models diverge the most\n",
    "\n",
    "**This notebook depends on:**\n",
    "* Running `./download_results.py` (PUBLIC USE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import urllib, io\n",
    "\n",
    "sys.path.append('./analysis_helpers')\n",
    "from importlib import reload\n",
    "from analysis_helpers import *\n",
    "from display_trials import *\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from IPython.display import Video\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import requests\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# seaborn plotting themes\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up paths and directoriesg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "video_dir = os.path.join(results_dir,'videos')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)   \n",
    "    \n",
    "if not os.path.exists(video_dir):\n",
    "    os.makedirs(video_dir) \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(analysis_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(analysis_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_meta import *\n",
    "HEM = pd.DataFrame(NEURIPS2021_EXPS) # HEM = \"human experiment metadata\"\n",
    "HEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get paths to all human response data\n",
    "data_paths = [os.path.join(csv_dir,'humans',i) for i in os.listdir(os.path.join(csv_dir,'humans'))]\n",
    "resp_paths = [i for i in data_paths if i.split('/')[-1].split('-')[0]=='human_responses']\n",
    "assert len(resp_paths)==8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in example dataframe\n",
    "exp_ind = 0\n",
    "d = pd.concat([pd.read_csv(p) for p in resp_paths])\n",
    "\n",
    "## some utility vars\n",
    "d['scenarioName'] = d['study'].apply(lambda x:x.split('_')[0])\n",
    "# colnames_with_variable_entries = [col for col in sorted(d.columns) if len(np.unique(d[col]))>1]\n",
    "colnames = ['scenarioName','study','gameID','trialNum','prolificIDAnon','stim_ID','response','target_hit_zone_label','correct','choices','rt']\n",
    "# colnames = ['gameID','trialNum','stim_ID','response','target_hit_zone_label','correct','choices','rt']\n",
    "\n",
    "## subset dataframe by colnames of interest\n",
    "_D = d[colnames]\n",
    "\n",
    "## preprocess RTs (subtract 2500ms presentation time, log transform)\n",
    "_D = _D.assign(RT = _D['rt'] - 2500) \n",
    "_D = _D.assign(logRT = np.log(_D['RT']))\n",
    "_D = _D.drop(columns=['rt'],axis=1)\n",
    "\n",
    "## convert responses to boolean\n",
    "binary_mapper = {'YES':True, 'NO':False}\n",
    "_D = _D.assign(responseBool = _D['response'].apply(lambda x: binary_mapper[x]), axis=0)\n",
    "\n",
    "# print('Currently analyzing the {} experiment.'.format(scenarioName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exclusion criteria (from `preregistration_neurips2021.md`)\n",
    "\n",
    " Data from an entire experimental session will be excluded if the responses:\n",
    " * contain a sequence with unusually long streak, defined as occurring less than 2.5% of the time under random responding\n",
    " * contain a sequence of at least 24 trials alternating \"yes\" and \"no\" responses\n",
    " * are correct for fewer than 4 out of 10 familiarization trials (i.e., 30% correct or lower)\n",
    " * the mean accuracy for that participant is below 3 standard deviations below the median accuracy across all participants \\for that scenario\n",
    " * the mean log-transformed response time for that participant is 3 standard deviations above the median log-transformed response time across all participants for that scenario\n",
    " \n",
    "Excluded sessions will be flagged. Flagged sessions will not be included in the main analyses. We will also conduct our planned analyses with the flagged sessions included to investigate the extent to which the outcomes of the main analyses change when these sessions are included. Specifically, we will fit a statistical model to all sessions and estimate the effect of a session being flagged on accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_helpers import *\n",
    "D = apply_exclusion_criteria(_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with all downloading urls for the mp4 videos in human experiments\n",
    "def_path = '../results/csv/'\n",
    "D_video = D[['study', 'stim_ID']]\n",
    "D_video = pd.merge(D_video, HEM, on = \"study\")\n",
    "\n",
    "D_video[\"bucket_name\"] = \"https://\" + D_video[\"bucket_name\"] + \".s3.amazonaws.com/\"\n",
    "D_video[\"url\"] = D_video[\"bucket_name\"] + D_video[\"stim_ID\"] + '.mp4'\n",
    "D_video[\"video_name\"] = D_video[\"stim_ID\"] + '.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get downlading urls for all human experiment trial videos\n",
    "video_path = os.path.join(os.path.dirname(os.getcwd()), 'results/videos')\n",
    "video_names =  D_video[\"video_name\"].unique() # all video names\n",
    "video_urls = D_video[\"url\"].unique() # all urls\n",
    "\n",
    "# iteratively download all videos\n",
    "for i in tqdm(range(len(video_urls))):\n",
    "    completeName = os.path.join(video_path, video_names[i])\n",
    "    if not os.path.isfile(completeName):\n",
    "        r = requests.get(video_urls[i], allow_redirects=True)\n",
    "        open(completeName, 'wb').write(r.content)\n",
    "\n",
    "# Or if fails, then download all mp4 files from this [link]\n",
    "# (https://physics-benchmarking-neurips2021-dataset.s3.amazonaws.com/Physion.zip) and put them into the `./result/videos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many mp4 we downloaded (should be 1200 [as 150 * 8 scenarios])\n",
    "#os.remove(os.path.join(video_path, '.ipynb_checkpoints'))\n",
    "file_names = [name for name in os.listdir(video_path)]\n",
    "video_count = sum([1 for name in file_names if str(name).endswith('.mp4')])\n",
    "print('Number of mp4 files downloaded: ' + str(video_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution and compute summary statistics over human physical judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy by scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dominoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "dominoes = D[D[\"scenarioName\"] == \"dominoes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the accuracy dsitribution\n",
    "Dacc = dominoes.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for dominoes')\n",
    "\n",
    "Dacc['answer'] = dominoes.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) tematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Total/ bad occlusion (problematic)\n",
    "- Block stuck at weird positions (problematic)\n",
    "- Just hard to judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support (Tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "support = D[D[\"scenarioName\"] == \"towers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = support.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for tower')\n",
    "\n",
    "Dacc['answer'] = support.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Fall but didn’t hit the yellow region\n",
    "- Weirdly stable\n",
    "- Weirdly unstable\n",
    "- Takes a long time to finalize (small perturbance in the beginning, should actually hit the ground eventually but take too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "collide = D[D[\"scenarioName\"] == \"collision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = collide.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for collison')\n",
    "\n",
    "Dacc['answer'] = collide.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Just very hard to tell\n",
    "- Wrong (early end, <= 5 sec is not sufficient)\n",
    "- Projectile hard to tell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "contain = D[D[\"scenarioName\"] == \"containment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = contain.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for tower')\n",
    "\n",
    "Dacc['answer'] = contain.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc[Dacc[\"correct\"]<=0.33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Hard to predict (the ball got stuck)\n",
    "- False output with respect to ground truth (problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "drop = D[D[\"scenarioName\"] == \"drop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = drop.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for drop')\n",
    "\n",
    "Dacc['answer'] = drop.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Relative distance to the center of the target object is hard to determine/ relative size of an object creates a false sense of distance \n",
    "- Fail to predict whether a object will stuck\n",
    "- Weird angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "link = D[D[\"scenarioName\"] == \"linking\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = link.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for linking')\n",
    "\n",
    "Dacc['answer'] = link.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Fail to also consider the ball will knock the pole closer to the yellow area (could add similar trials in familiarization)\n",
    "- Ball hitting way too hard (maybe reduce ones with short pole and few rings or add familiarization trials for those)\n",
    "- lots of people also failed on ones with inverse hula hoop size arrangement, not sure why (7/ 24 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dominoes trials\n",
    "roll = D[D[\"scenarioName\"] == \"rollingsliding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = roll.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for rolling & sliding')\n",
    "\n",
    "Dacc['answer'] = roll.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Broken generation with no slope (problematic)\n",
    "- Potentially way too easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all drape trials\n",
    "drape = D[D[\"scenarioName\"] == \"clothiness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33.3%/ 66.7%/ 100% Accuracy Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dacc = drape.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli for drape')\n",
    "\n",
    "Dacc['answer'] = drape.groupby('stim_ID')['target_hit_zone_label'].first() # add ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) systematically fail (0 <= p < 33.3%)\n",
    "Instances with low accuracy:\n",
    "- Can’t tell there is contact even after watching the whole video (problematic)\n",
    "    - All claim there is a hit, but can’t really tell (not sure if physics broke or didn’t define the point of contact properly)\n",
    "- Simply hard to tell if there is not a hit\n",
    "- Looks like the physics engine is broken in two instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies lower than 33.3%\n",
    "plot_by_n(Dacc, video_dir, 0.0, 0.333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) are close to chance (33.3% <= p < 66.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies between 33.3% and 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.333, 0.667, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) consistently succeed (66.7% <= p <= 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trails with accuracies above 66.7%\n",
    "plot_by_n(Dacc, video_dir, 0.667, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up video downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up all the downloaded mp4 files in `./results/videos` folder\n",
    "filelist = glob.glob(os.path.join(video_path, \"*\"))\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of stimulus qualities that are problematic. \n",
    "- Bad occlusion (can't see any domino, or portion important for inference is blocked)\n",
    "- Broken physics (sometimes the result is contradictory to observation or doesn't follow physical law)\n",
    "- Process take too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
