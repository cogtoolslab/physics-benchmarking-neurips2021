{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2ae1e6",
   "metadata": {},
   "source": [
    "# Paper plots\n",
    "\n",
    "This notebook generates the plots used in the paper.  \n",
    "\n",
    "**This notebook depends on:**\n",
    "* Running `./summarize_human_model_behavior.ipynb` (PUBLIC USE)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef1156",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1ebb1",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99bd493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "\n",
    "sys.path.append('./analysis_helpers')\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import analysis_helpers as h\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4651a",
   "metadata": {},
   "source": [
    "#### options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e2bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# seaborn plotting themes\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9baab4d",
   "metadata": {},
   "source": [
    "#### set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412e0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(analysis_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(analysis_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51dcd7",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459f823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AMH = pd.read_csv(os.path.join(csv_dir,\"summary/model_human_accuracies.csv\"))\n",
    "AMCK = pd.read_csv(os.path.join(csv_dir,\"summary/model_human_CohensK.csv\"))\n",
    "AMRMSE = pd.read_csv(os.path.join(csv_dir,\"summary/model_human_pearsonsr_rmse.csv\"))\n",
    "AH = pd.read_csv(os.path.join(csv_dir,\"summary/human_accuracy_by_scenario.csv\"))\n",
    "ACK = pd.read_csv(os.path.join(csv_dir,\"summary/human_pairwiseCohensKs_by_scenario.csv\"))\n",
    "AR = pd.read_csv(os.path.join(csv_dir,\"summary/human_pairwiseCorrs_by_scenario.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fed02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get percentiles for AMRMSE\n",
    "# AMRMSE = AMRMSE.groupby(['ModelID','scenario']).agg({\n",
    "#     'Model Kind':'first',\n",
    "#     'RMSE':['mean',lambda x:np.percentile(x,2.5),lambda x:np.percentile(x,97.5)]\n",
    "# })\n",
    "\n",
    "# # get rid of multicols\n",
    "# AMRMSE.columns = [''.join(col) if type(col) is tuple else col for col in AMRMSE.columns.values]\n",
    "\n",
    "# AMRMSE.reset_index(inplace=True)\n",
    "\n",
    "# AMRMSE = AMRMSE.rename(columns={\n",
    "#     \"RMSEmean\":\"RMSE_mean\",\n",
    "#     \"RMSE<lambda_0>\":\"RMSE_lb\",\n",
    "#     \"RMSE<lambda_1>\":\"RMSE_ub\",\n",
    "#     \"Model Kindfirst\":\"Model Kind\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9108efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "_AMCK = AMCK[[\"ModelID\",\"scenario\",\"Cohens_k_lb\",\"Cohens_k_med\",\"Cohens_k_ub\"]]\n",
    "# _AMRMSE = AMRMSE[[\"ModelID\",\"scenario\",\"RMSE_mean\",\"RMSE_lb\",\"RMSE_ub\",]]\n",
    "_AMRMSE = AMRMSE[[\"ModelID\",\"scenario\",\"RMSE\",\"pearsons_r\"]]\n",
    "MH = pd.merge(AMH,_AMCK,on=['ModelID',\"scenario\"])\n",
    "MH = pd.merge(MH,_AMRMSE,on=['ModelID',\"scenario\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542270c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCENARIOS = sorted(MH['scenario'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e759195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kick out SVG/RPIN additional seeds\n",
    "MH = MH[~(((MH['Model'] == 'SVG') | (MH['Model'] == 'RPIN')) & (MH['Encoder Training Seed'] != 0.0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80fcfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#choose readout and training type\n",
    "MH_both = MH[(MH['Readout Type'] == \"B\") & (MH['Dynamics Training Dataset Type'] == \"all\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454667b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MH_readouts = MH[(MH['Dynamics Training Dataset Type'] == \"all\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6cd8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MH_trainings = MH[(MH['Readout Type'] == \"B\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60d09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#collapse model names HACK\n",
    "MH_trainings['Model Kind'] = MH_trainings['Model Kind'].apply(lambda x: x.replace(\"same_\",\"all_\"))\n",
    "MH_trainings['Model Kind'] = MH_trainings['Model Kind'].apply(lambda x: x.replace(\"all_but_this\",\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc757d12",
   "metadata": {},
   "source": [
    "## Model/human comparision plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c8734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set up model names\n",
    "MODELS = dict(MH_both.groupby('Model Kind')['Model'].first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e81d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.all(MH_both.groupby('Model Kind')['ModelID'].count() == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28cd8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename these by hand if necessary\n",
    "MODELS = {\n",
    " 'SVG_VGG_0.0_VAE_all_VAE_0_same': 'SVG',\n",
    " 'OP3_OP3 encoder_0.0_Image Reconstruction_all_Image Reconstruction_0_same': 'OP3',\n",
    " 'CSWM_CSWM encoder_0.0_Contrastive_all_Contrastive_0_same': 'CSWM',\n",
    " 'RPIN_R-CNN_0.0_L2 on 2D position_all_L2 on 2D position_0_same': 'RPIN',\n",
    " 'VGGFrozenMLP_VGG_nan_nan_nan_L2 on latent_0_same': 'pVGG-mlp',\n",
    " 'VGGFrozenLSTM_VGG_nan_nan_nan_L2 on latent_0_same': 'pVGG-lstm',\n",
    " 'DEITFrozenMLP_DEIT_nan_nan_nan_L2 on latent_0_same': 'pDEIT-mlp',\n",
    " 'DEITFrozenLSTM_DEIT_nan_nan_nan_L2 on latent_0_same': 'pDEIT-lstm',\n",
    " 'GNS_nan_nan_nan_nan_L2 on particle 3D positions_1_same': 'GNS',\n",
    " 'GNS-ransac_nan_nan_nan_nan_L2 on particle 3D positions_1_same': 'GNS-R',\n",
    " 'DPI_nan_nan_nan_nan_L2 on particle 3D positions_1_same': 'DPI',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9eaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COLORS = {\n",
    "    'SVG':'orangered',\n",
    "    'OP3':'darkorange',\n",
    "    'CSWM':'gold',\n",
    "    'RPIN':'greenyellow',\n",
    "    'pDEIT-lstm':'royalblue',\n",
    "    'pDEIT-mlp':'blue',\n",
    "    'pVGG-lstm':'aquamarine',\n",
    "    'pVGG-mlp':'turquoise',\n",
    "    'GNS':'slateblue',\n",
    "    'GNS-R':'orchid',\n",
    "    'DPI':'hotpink',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ddc57",
   "metadata": {},
   "source": [
    "### Pretty plotting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bad80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from svgpathtools import svg2paths\n",
    "from svgpath2mpl import parse_path\n",
    "def marker_from_svg(path):\n",
    "    path, attributes = svg2paths(path)\n",
    "    marker = parse_path(attributes[0]['d'])\n",
    "#     marker = parse_path(path)\n",
    "    marker.vertices -= marker.vertices.mean(axis=0)\n",
    "    marker = marker.transformed(mpl.transforms.Affine2D().rotate_deg(180))\n",
    "    marker = marker.transformed(mpl.transforms.Affine2D().scale(-1,1))\n",
    "    return marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f05746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ICONS = {\n",
    "     'clothiness':\"<\",\n",
    "     'collision':\">\",\n",
    "     'containment':\"X\",\n",
    "     'dominoes':\"p\",\n",
    "     'drop':\"d\",\n",
    "     'linking':\"h\",\n",
    "     'rollingsliding':\"v\",\n",
    "     'towers':\"P\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93bd41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svg_dir = os.path.join(proj_dir,\"figures/icons/SVG\")\n",
    "\n",
    "ICONS = {\n",
    "     'clothiness':marker_from_svg(os.path.join(svg_dir,\"clothiness.svg\")),\n",
    "     'collision':marker_from_svg(os.path.join(svg_dir,\"collision.svg\")),\n",
    "     'containment':marker_from_svg(os.path.join(svg_dir,\"containment.svg\")),\n",
    "     'dominoes':marker_from_svg(os.path.join(svg_dir,\"dominoes.svg\")),\n",
    "     'drop':marker_from_svg(os.path.join(svg_dir,\"drop.svg\")),\n",
    "     'linking':marker_from_svg(os.path.join(svg_dir,\"linking.svg\")),\n",
    "     'rollingsliding':marker_from_svg(os.path.join(svg_dir,\"rollingsliding.svg\")),\n",
    "     'towers':marker_from_svg(os.path.join(svg_dir,\"towers.svg\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f7272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set up color map over models\n",
    "from matplotlib import cm\n",
    "\n",
    "cmap = cm.get_cmap('tab20_r')\n",
    "# cmap = cmap(np.linspace(0, 1, 22))\n",
    "cmap = cmap.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c1888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MARKER_SIZE = 20\n",
    "MEAN_MULTIPLIER = 0.8\n",
    "ALPHA = 0.2\n",
    "SCEN_SHIFT = .06\n",
    "N_ITER = 1000\n",
    "FIGSIZE = (7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed3a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap_means(means):\n",
    "    \"\"\"Bootstraps over the means across scenarios to generate lower and upper confidence bounds\"\"\"\n",
    "    bootstrapped_means = [np.mean(np.random.choice(means,size=len(AH),replace=True)) for _ in range(N_ITER)]\n",
    "    lb = np.percentile(bootstrapped_means,2.5)\n",
    "    ub = np.percentile(bootstrapped_means,97.5)\n",
    "    mean = np.mean(means)\n",
    "    boot_mean = np.mean(bootstrapped_means)\n",
    "    boot_median = np.percentile(bootstrapped_means,50)\n",
    "    return mean,lb,ub,boot_mean,boot_median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8eee99",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37187c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "# human zone\n",
    "## human\n",
    "mean_human,lb_human,ub_human,_,_ = bootstrap_means(AH['obs_mean'])\n",
    "axis.axhspan(lb_human,ub_human,\n",
    "              color=\"black\",\n",
    "               alpha=0.1)\n",
    "\n",
    "for si,scenario in enumerate(SCENARIOS):\n",
    "    #plot human data\n",
    "    axis.plot(0+si*.03,AH[AH['scenario']==scenario]['obs_mean'],\n",
    "              label=\"Human\",\n",
    "              marker=ICONS[scenario],\n",
    "              fillstyle='full',\n",
    "#               markerfacecolor=\"black\",\n",
    "              color=\"black\",\n",
    "              alpha=.3, \n",
    "              ms=MARKER_SIZE)\n",
    "    #plot model data\n",
    "    for x,(model,model_name) in enumerate(MODELS.items()):\n",
    "        axis.plot(x+1+si*SCEN_SHIFT,MH_both[(MH_both['scenario']==scenario) & (MH_both['Model Kind'] == model)]['model_correct'],\n",
    "                  marker=ICONS[scenario],\n",
    "                  fillstyle='full',\n",
    "                  markerfacecolor=COLORS[model_name],\n",
    "                  color=\"black\",\n",
    "                  label=scenario,\n",
    "                  alpha=.3,\n",
    "                  ms=MARKER_SIZE)\n",
    "#plot means\n",
    "## human\n",
    "lb_human = abs(mean_human - lb_human) #as we error relative to mean\n",
    "ub_human = abs(mean_human - ub_human)\n",
    "axis.errorbar(0-.2,mean_human,\n",
    "              yerr=([lb_human],[ub_human]),\n",
    "              label=\"Human\",\n",
    "              color=\"black\",\n",
    "              marker = \"o\",\n",
    "              ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "## models\n",
    "for x,(model,model_name) in enumerate(MODELS.items()):\n",
    "    mean,lb,ub,_,_ = bootstrap_means(MH_both[(MH_both['Model Kind'] == model)]['model_correct'])\n",
    "    axis.errorbar(x+1-.2,mean,([abs(mean-lb)],[abs(mean-ub)]),\n",
    "      marker=\"o\",\n",
    "      color=COLORS[model_name],\n",
    "      label=model_name,\n",
    "      ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "\n",
    "#add labels\n",
    "axis.set_xticks(np.arange(len(MODELS)+1))\n",
    "axis.set_xticklabels([\"Human\"] + list(MODELS.values()),rotation=45,ha='right')\n",
    "axis.set_title(\"Accuracy\")\n",
    "axis.set_ylabel(\"Accuracy\")\n",
    "axis.set_ybound((0.3,1))\n",
    "plt.savefig(os.path.join(plot_dir,\"human_model_accuracy.pdf\"),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d841acf",
   "metadata": {},
   "source": [
    "### Cohen's $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fd286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "# human zone\n",
    "## human\n",
    "mean_human,lb_human,ub_human,_,_ = bootstrap_means(ACK['corr_med'])\n",
    "axis.axhspan(lb_human,ub_human,\n",
    "              color=\"black\",\n",
    "               alpha=0.1)\n",
    "\n",
    "for si,scenario in enumerate(SCENARIOS):\n",
    "    #plot human data\n",
    "    axis.plot(0+si*.03,ACK[ACK['scenario']==scenario]['corr_med'],\n",
    "              label=\"Human\",\n",
    "              marker=ICONS[scenario],\n",
    "              fillstyle='full',\n",
    "#               markerfacecolor=\"black\",\n",
    "              color=\"black\",\n",
    "              alpha=.3, \n",
    "              ms=MARKER_SIZE)\n",
    "    #plot model data\n",
    "    for x,(model,model_name) in enumerate(MODELS.items()):\n",
    "        axis.plot(x+1+si*SCEN_SHIFT,MH_both[(MH_both['scenario']==scenario) & (MH_both['Model Kind'] == model)]['Cohens_k_med'],\n",
    "                  marker=ICONS[scenario],\n",
    "                  fillstyle='full',\n",
    "                  markerfacecolor=COLORS[model_name],\n",
    "                  color=\"black\",\n",
    "                  label=model_name,\n",
    "                  alpha=.3,\n",
    "                  ms=MARKER_SIZE)\n",
    "#plot means\n",
    "## human\n",
    "lb_human = abs(mean_human - lb_human) #as we error relative to mean\n",
    "ub_human = abs(mean_human - ub_human)\n",
    "axis.errorbar(0-.2,mean_human,\n",
    "              yerr=([lb_human],[ub_human]),\n",
    "              label=\"Human\",\n",
    "              color=\"black\",\n",
    "              marker = \"o\",\n",
    "              ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "## models\n",
    "for x,(model,model_name) in enumerate(MODELS.items()):\n",
    "    mean,lb,ub,_,_ = bootstrap_means(MH_both[(MH_both['Model Kind'] == model)]['Cohens_k_med'])\n",
    "    axis.errorbar(x+1-.2,mean,([abs(mean-lb)],[abs(mean-ub)]),\n",
    "      marker=\"o\",\n",
    "      color=COLORS[model_name],\n",
    "      label=model_name,\n",
    "      ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "   \n",
    "#add labels\n",
    "axis.set_xticks(np.arange(len(MODELS)+1))\n",
    "axis.set_xticklabels([\"Human\"] + list(MODELS.values()),rotation=45,ha='right')\n",
    "axis.set_title(\"Cohen's $\\kappa$\")\n",
    "axis.set_ylabel(\"$\\kappa$ within human/between human & model\")\n",
    "axis.set_ybound((-.25,1))\n",
    "plt.savefig(os.path.join(plot_dir,\"human_model_CohensK.pdf\"),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c955d7",
   "metadata": {},
   "source": [
    "### Correlation between model & average human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a56a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "# human zone\n",
    "lb = AR['r_lb'].mean()\n",
    "ub = AR['r_ub'].mean()\n",
    "axis.axhspan(lb,ub,\n",
    "              color=\"black\",\n",
    "               alpha=0.1)\n",
    "\n",
    "for si,scenario in enumerate(SCENARIOS):\n",
    "    #plot human data\n",
    "    axis.plot(0+si*.03,AR[AR['scenario']==scenario]['r_mean'],\n",
    "              label=\"Human\",\n",
    "              marker=ICONS[scenario],\n",
    "              fillstyle='full',\n",
    "#               markerfacecolor=\"black\",\n",
    "              color=\"black\",\n",
    "              alpha=.3, \n",
    "              ms=MARKER_SIZE)\n",
    "    #plot model data\n",
    "    for x,(model,model_name) in enumerate(MODELS.items()):\n",
    "        axis.plot(x+1+si*SCEN_SHIFT,MH_both[(MH_both['scenario']==scenario) & (MH_both['Model Kind'] == model)]['pearsons_r'],\n",
    "                  marker=ICONS[scenario],\n",
    "                  fillstyle='full',\n",
    "                  markerfacecolor=COLORS[model_name],\n",
    "                  color=\"black\",\n",
    "                  label=model_name,\n",
    "                  alpha=.3,\n",
    "                  ms=MARKER_SIZE)\n",
    "# plot means\n",
    "# human\n",
    "mean = AR['r_mean'].mean()\n",
    "lb = abs(mean-AR['r_lb'].mean())\n",
    "ub = abs(mean-AR['r_ub'].mean())\n",
    "assert AR['r_ub'].mean() == ub+mean\n",
    "axis.errorbar(0-.2,mean,\n",
    "              yerr=([lb],[ub]),\n",
    "              label=\"Human\",\n",
    "              color=\"black\",\n",
    "              marker = \"o\",\n",
    "              ms=MARKER_SIZE*MEAN_MULTIPLIER,\n",
    ")\n",
    "## models\n",
    "for x,(model,model_name) in enumerate(MODELS.items()):\n",
    "    print(\"error bar\",model_name,model)\n",
    "    pearsons_rs = MH_both[(MH_both['Model Kind'] == model)]['pearsons_r']\n",
    "    # drop nans\n",
    "    pearsons_rs = pearsons_rs[~np.isnan(pearsons_rs)]\n",
    "    mean,lb,ub,_,_ = bootstrap_means(pearsons_rs)\n",
    "    print(mean,lb,ub)\n",
    "    axis.errorbar(x+1+-.2,mean,([abs(mean-lb)],[abs(mean-ub)]),\n",
    "      marker=\"o\",\n",
    "      color=COLORS[model_name],\n",
    "      label=model_name,\n",
    "      ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "   \n",
    "#add labels\n",
    "axis.set_xticks(np.arange(len(MODELS)+1))\n",
    "axis.set_xticklabels([\"Human\"] + list(MODELS.values()),rotation=45,ha='right')\n",
    "axis.set_title(\"Correlation to average human response\")\n",
    "axis.set_ylabel(\"Pearson's r\")\n",
    "# axis.set_ybound((0,0.08))\n",
    "plt.savefig(os.path.join(plot_dir,\"human_model_distance.pdf\"),bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d51fd-123c-4c99-bf49-46ff2a12b173",
   "metadata": {},
   "source": [
    "## Adversarial stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ee2c1-78af-4a90-a7a6-386361c9fa9d",
   "metadata": {},
   "source": [
    "### Model/human correlation in full vs adversarial stim set\n",
    "Assumes that `summary/model_human_pearsonsr_rmse_ADVERSARIAL.csv` contains a parallel version of the data in `summary/model_human_pearsonsr_rmse.csv`, but only with adversarial stimuli (ie. those that humans get wrong in >50% of cases). Use `summarize_human_model_behavior.ipynb` to generate the .csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3dbbe-090e-4aec-8485-7d38a63a09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in adversarial only correlations\n",
    "AMRMSE = pd.read_csv(os.path.join(csv_dir,\"summary/model_human_pearsonsr_rmse.csv\"))\n",
    "AMRMSE_adversarial = pd.read_csv(os.path.join(csv_dir,\"summary/model_human_pearsonsr_rmse_ADVERSARIAL.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d938341-08b0-4d35-8c26-24d7350472c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kick out SVG/RPIN additional seeds\n",
    "AMRMSE = AMRMSE[~(((AMRMSE['Model'] == 'SVG') | (AMRMSE['Model'] == 'RPIN')) & (AMRMSE['Encoder Training Seed'] != 0.0))]\n",
    "AMRMSE_adversarial = AMRMSE_adversarial[~(((AMRMSE_adversarial['Model'] == 'SVG') | (AMRMSE_adversarial['Model'] == 'RPIN')) & (AMRMSE_adversarial['Encoder Training Seed'] != 0.0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb3839-7159-43fe-9fea-128c7e562353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#choose readout and training type\n",
    "AMRMSE = AMRMSE[(AMRMSE['Readout Type'] == \"B\") & (AMRMSE['Dynamics Training Dataset Type'] == \"all\")]\n",
    "AMRMSE_adversarial = AMRMSE_adversarial[(AMRMSE_adversarial['Readout Type'] == \"B\") & (AMRMSE_adversarial['Dynamics Training Dataset Type'] == \"all\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bdc09-f6fd-4fe1-ad06-b62fc4cde0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "for mi,(model,model_name) in enumerate(MODELS.items()):\n",
    "    for scenario in SCENARIOS:\n",
    "        axis.plot(AMRMSE[(AMRMSE['scenario'] == scenario) & (AMRMSE['Model Kind'] == model)]['pearsons_r'].head(1), #head is an ugly hack for duplicates\n",
    "                  AMRMSE_adversarial[(AMRMSE_adversarial['scenario'] == scenario) & (AMRMSE_adversarial['Model Kind'] == model)]['pearsons_r'],\n",
    "                  marker=ICONS[scenario],\n",
    "                  fillstyle='full',\n",
    "                  markerfacecolor=COLORS[model_name],\n",
    "                  color=\"black\",\n",
    "                  label=model_name,\n",
    "                  alpha=.3,\n",
    "                  ms=MARKER_SIZE)\n",
    "        #add colored text label\n",
    "        fig.text(.95,.15+mi*1/15,model_name,color=COLORS[model_name],alpha=.1,fontsize='small',fontweight='bold')\n",
    "# plt.legend()\n",
    "plt.xlabel(\"Pearson's r on all stimuli\")\n",
    "plt.ylabel(\"Pearson's r on adversarial stimuli\")\n",
    "plt.title(\"Pearson's r between model and humans\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b3c46",
   "metadata": {},
   "source": [
    "### Readout type\n",
    "only vision models\n",
    "\n",
    "view + simulation is not better than just view\n",
    "\n",
    "compare B & C on per model basis\n",
    "paired lines\n",
    "average across scenario\n",
    "col: model\n",
    "    subcol0 C\n",
    "    subcol1 B\n",
    "    subcol2 A\n",
    "y accuracy\n",
    "color: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5cd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the models we have more than one readout type for\n",
    "readout_models = [m for m,c in (MH_readouts.groupby('Model Kind')['Readout Type'].nunique()).items() if c >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883f77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODELS_readout = {key:value for key,value in MODELS.items() if key in readout_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2482ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "READOUTS = [\"C\",\"B\",\"A\"]\n",
    "\n",
    "READOUT_MARKER = {\n",
    "    'A': '^',\n",
    "    'B': 'o',\n",
    "    'C': 's'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337ddab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=(9,3))\n",
    "\n",
    "# human zone\n",
    "## human\n",
    "mean_human,lb_human,ub_human,_,_ = bootstrap_means(AH['obs_mean'])\n",
    "axis.axhspan(lb_human,ub_human,\n",
    "              color=\"black\",\n",
    "               alpha=0.1)\n",
    "\n",
    "#plot means\n",
    "## human\n",
    "lb_human = abs(mean_human - lb_human) #as we error relative to mean\n",
    "ub_human = abs(mean_human - ub_human)\n",
    "axis.errorbar(0-.2,mean_human,\n",
    "              yerr=([lb_human],[ub_human]),\n",
    "              label=\"Human\",\n",
    "              color=\"black\",\n",
    "              marker = \"o\",\n",
    "              ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "## models\n",
    "for x,(model,model_name) in enumerate(MODELS_readout.items()):\n",
    "    means = []\n",
    "    lbs = []\n",
    "    ubs = []\n",
    "    for o,readout in enumerate(READOUTS):\n",
    "        mean,lb,ub,_,_ = bootstrap_means(MH_readouts[(MH_readouts['Model Kind'] == model)&\n",
    "                                                     (MH_readouts['Readout Type'] == readout)]['model_correct'])\n",
    "        lb = abs(mean-lb); ub = abs(mean-ub); \n",
    "        means.append(mean); lbs.append(lb), ubs.append(ub)\n",
    "        axis.errorbar(x+.66+0.33 * o,mean,([lb],[ub]),\n",
    "          marker=READOUT_MARKER[readout],\n",
    "          color=COLORS[model_name],\n",
    "          label=model_name,\n",
    "          ms=10)\n",
    "    plt.plot([x+.66+0.33 * o for o in range(len(means))],means,color=COLORS[model_name])\n",
    "\n",
    "#add labels\n",
    "axis.set_xticks(np.arange(len(MODELS_readout)+1))\n",
    "axis.set_xticklabels([\"Human\"] + list(MODELS_readout.values()),rotation=45,ha='right')\n",
    "axis.set_title(\"Accuracy per readout type\")\n",
    "axis.set_ylabel(\"Accuracy\")\n",
    "axis.set_ybound((0.3,1))\n",
    "plt.savefig(os.path.join(plot_dir,\"human_model_readout_accuracy.pdf\"),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4a9de",
   "metadata": {},
   "source": [
    "### Training Type\n",
    "do some models generalize better than others? Are any of the scenarios more important to include\n",
    "points for scenarios\n",
    "maybe: same as cohens with interconnected lines\n",
    "or paired barplot with errorbars\n",
    "\n",
    "or two plots: connected lines\n",
    "* averaging across scenarios (are some models better at generalizing?)\n",
    "* are there scenarios that are important to include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19b65c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the models we have more than one readout type for\n",
    "trainings_models = [m for m,c in (MH_trainings.groupby('Model Kind')['Dynamics Training Dataset Type'].nunique()).items() if c >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18096b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODELS_trainings = {key:value for key,value in MODELS.items() if key in trainings_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c595a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAININGS = [\"all\",\"all_but_this\",\"same\"]\n",
    "\n",
    "TRAINING_FILL = {\n",
    "    'all': 'full',\n",
    "    'all_but_this': 'left',\n",
    "    'same': 'none'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617b183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=(9,3))\n",
    "\n",
    "# human zone\n",
    "## human\n",
    "mean_human,lb_human,ub_human,_,_ = bootstrap_means(AH['obs_mean'])\n",
    "axis.axhspan(lb_human,ub_human,\n",
    "              color=\"black\",\n",
    "               alpha=0.1)\n",
    "\n",
    "#plot means\n",
    "## human\n",
    "lb_human = abs(mean_human - lb_human) #as we error relative to mean\n",
    "ub_human = abs(mean_human - ub_human)\n",
    "axis.errorbar(0-.2,mean_human,\n",
    "              yerr=([lb_human],[ub_human]),\n",
    "              label=\"Human\",\n",
    "              color=\"black\",\n",
    "              marker = \"o\",\n",
    "              ms=MARKER_SIZE*MEAN_MULTIPLIER)\n",
    "## models\n",
    "for x,(model,model_name) in enumerate(MODELS_trainings.items()):\n",
    "    means = []\n",
    "    lbs = []\n",
    "    ubs = []\n",
    "    for o,training in enumerate(TRAININGS):\n",
    "        mean,lb,ub,_,_ = bootstrap_means(MH_trainings[(MH_trainings['Model Kind'] == model)&\n",
    "                                                     (MH_trainings['Dynamics Training Dataset Type'] == training)]['model_correct'])\n",
    "        lb = abs(mean-lb); ub = abs(mean-ub); \n",
    "        means.append(mean); lbs.append(lb), ubs.append(ub)\n",
    "        axis.errorbar(x+.66+0.33 * o,mean,([lb],[ub]),\n",
    "          marker=\"o\",\n",
    "          fillstyle=TRAINING_FILL[training],\n",
    "          color=COLORS[model_name],\n",
    "          label=model_name,\n",
    "          ms=10)\n",
    "    plt.plot([x+.66+0.33 * o for o in range(len(means))],means,color=COLORS[model_name])\n",
    "\n",
    "#add labels\n",
    "axis.set_xticks(np.arange(len(MODELS_trainings)+1))\n",
    "axis.set_xticklabels([\"Human\"] + list(MODELS_trainings.values()),rotation=45,ha='right')\n",
    "axis.set_title(\"Accuracy per dynamics training regimen\")\n",
    "axis.set_ylabel(\"Accuracy\")\n",
    "axis.set_ybound((0.3,1))\n",
    "plt.savefig(os.path.join(plot_dir,\"human_model_training_accuracy.pdf\"),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a906253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032413f3",
   "metadata": {},
   "source": [
    "## Scatter human/model over scenario\n",
    "since DPI seems to be the most human-like (or maybe we could do this for a few of the models), we could look more finely at which scenarios it's making human or non-human errors on. So this could be a scatter plot with human performance on y, model performance on x, with one \"dot\" per scenario (i.e. using the colored icons.) Deviation from the line y = x is easily visible and indicates that the models still arent' making human-like predictions (+ whether that's because they're more or less accurate.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82f85c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"DPI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99143f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MH_scatter = MH_both[MH_both['Model'] == model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f534ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "#discrinimation line\n",
    "axis.plot([0.5,1.],[0.5,1.],color=\"lightgrey\")\n",
    "\n",
    "for scenario in MH_scatter['scenario'].unique():\n",
    "    row = MH_scatter[MH_scatter['scenario'] == scenario]\n",
    "    axis.plot(row['model_correct'],\n",
    "                row['human_correct'],\n",
    "                marker = ICONS[scenario],\n",
    "                color=\"black\",\n",
    "                markerfacecolor=\"lightgrey\",\n",
    "                ms=30)\n",
    "    \n",
    "axis.set_xbound((0.5,1))\n",
    "axis.set_ybound((0.5,1))\n",
    "axis.set_xlabel(model+\" accuracy\")\n",
    "axis.set_ylabel(\"Human accuracy\")\n",
    "axis.set_title(\"Accuracy\")\n",
    "plt.savefig(os.path.join(plot_dir,\"human_{}_accuracy_comp.pdf\".format(model)),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf1a19",
   "metadata": {},
   "source": [
    "## Scatter human/model over individual datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8fc3d",
   "metadata": {},
   "source": [
    "### load human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281ab2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment_meta import *\n",
    "HEM = pd.DataFrame(NEURIPS2021_EXPS) # HEM = \"human experiment metadata\"\n",
    "HEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234304db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCENARIOS = sorted([n.split(\"_\")[0] for n in HEM['study'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb811b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get paths to all human response data\n",
    "data_paths = [os.path.join(csv_dir,'humans',i) for i in os.listdir(os.path.join(csv_dir,'humans'))]\n",
    "resp_paths = [i for i in data_paths if i.split('/')[-1].split('-')[0]=='human_responses']\n",
    "assert len(resp_paths)==8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfe6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## also load all human data into a big dataframe\n",
    "HD = pd.concat([h.apply_exclusion_criteria(h.load_and_preprocess_data(p)) for p in resp_paths])\n",
    "print(\"Loaded {} lines\".format(len(HD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fbebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### exclude subjects from familiarization\n",
    "Run `familiariarization_exclusion.ipynb` to generate `excluded_games.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc8cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_games = pd.read_csv(os.path.join(csv_dir,\"humans/excluded_games.csv\")).values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a8d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Excluding {} rows for {} games\".format(sum(HD['gameID'].isin(bad_games)), len(bad_games)))\n",
    "HD = HD[~HD['gameID'].isin(bad_games)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58d7fc",
   "metadata": {},
   "source": [
    "### load model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416ff04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get paths to all model data\n",
    "model_data_paths = [os.path.join(csv_dir,'models',i) for i in os.listdir(os.path.join(csv_dir,'models'))]\n",
    "model_res_paths = [i for i in model_data_paths if i.split('.')[-1] == \"csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569675a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load all model results into a single dataframe\n",
    "MD = pd.concat([pd.read_csv(p).assign(filename=p.split('/')[-1]) for p in model_res_paths])\n",
    "print(\"Loaded {} rows\".format(len(MD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b4f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a couple of import steps (restore original scenario names, add single prediction value, add correctness column)\n",
    "MD = h.process_model_dataframe(MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb6695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check for duplicated rows\n",
    "if len(MD.duplicated()) > 0:\n",
    "    print(\"⚠️There are {} duplicated rows!\".format(np.sum(MD.duplicated())))\n",
    "    MD = MD[~MD.duplicated(h.MODEL_COLS+[\"Stimulus Name\"],keep=\"first\")]\n",
    "    print(\"Removed duplicates, {} rows left\".format(len(MD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2f7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model kinds to variable\n",
    "MODELS = list(MD[\"Model Kind\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947982a",
   "metadata": {},
   "source": [
    "#### exclude bad stims (where model/human stims mismatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc16241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stim_comparision = pd.merge(pd.DataFrame(MD.groupby('Canon Stimulus Name')['Actual Outcome'].first()).reset_index(),pd.DataFrame(HD.groupby('stim_ID')['target_hit_zone_label'].first()).reset_index(),left_on='Canon Stimulus Name',right_on='stim_ID')\n",
    "\n",
    "bad_stims = stim_comparision[stim_comparision['Actual Outcome'] != stim_comparision['target_hit_zone_label']]['Canon Stimulus Name']\n",
    "print(\"There are {} bad stims\".format(len(bad_stims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d997514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Exclude bad stims\n",
    "HD = HD[~HD['stim_ID'].isin(bad_stims)]\n",
    "MD = MD[~MD['Canon Stimulus Name'].isin(bad_stims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df19fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Also exclude stims from the rollingsliding ledge subset\n",
    "HD = HD[~HD['stim_ID'].str.contains(\"rollingSliding_simple_ledge\")]\n",
    "MD = MD[~MD['Canon Stimulus Name'].str.contains(\"rollingSliding_simple_ledge\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cc2fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:17.889653Z",
     "iopub.status.busy": "2021-06-07T00:10:17.889415Z",
     "iopub.status.idle": "2021-06-07T00:10:17.892854Z",
     "shell.execute_reply": "2021-06-07T00:10:17.892014Z",
     "shell.execute_reply.started": "2021-06-07T00:10:17.889628Z"
    }
   },
   "source": [
    "### The plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d633c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MD['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ea60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_both = MD[(MD['Readout Type'] == \"B\") & (MD['Dynamics Training Dataset Type'] == \"all\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d107b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2fb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=4,figsize=(20,10),sharex=True,sharey=True)\n",
    "\n",
    "for i,scenario in enumerate(SCENARIOS):\n",
    "    axis = axes[int(i/4),i % 4]\n",
    "    axis.plot([0.,1.],[0.,1.],color='lightgrey')\n",
    "    human_responses = HD[HD['scenarioName']==scenario].groupby('stim_ID')['correct'].mean()\n",
    "    stims = human_responses.index\n",
    "    model_responses = MD_both[(MD_both['Canon Stimulus Name'].isin(stims)) & (MD_both['Model Kind'] == \"DPI_nan_nan_nan_nan_L2 on particle 3D positions_1_same\")].groupby('Canon Stimulus Name')['correct'].mean()\n",
    "    model_responses.sort_index(inplace=True)\n",
    "    human_responses.sort_index(inplace=True)\n",
    "    assert np.all(model_responses.index == human_responses.index)\n",
    "    #plot\n",
    "    axis.scatter(model_responses,\n",
    "                human_responses,\n",
    "                color=\"black\",s=10)\n",
    "    axis.set_title(\" \")  \n",
    "    axis.set_xlabel(model+\" accuracy\")\n",
    "    axis.set_ylabel(\"Human accuracy\")\n",
    "plt.savefig(os.path.join(plot_dir,\"human_{}_accuracy_per_stim.pdf\".format(model)),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79541aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the most divergent stims\n",
    "scenario = \"dominoes\"\n",
    "human_responses = HD[HD['scenarioName']==scenario].groupby('stim_ID')['responseBool'].mean()\n",
    "stims = human_responses.index\n",
    "model_responses = MD[MD['Canon Stimulus Name'].isin(stims)].groupby('Canon Stimulus Name')['Predicted Outcome'].mean()\n",
    "pd.DataFrame(abs(human_responses-model_responses)).sort_values(0).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47061c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "485befcfdfc6cbc3686511ea83990961fe5dedc3fb1316054ee2ccf541f5ee49"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tdw]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
