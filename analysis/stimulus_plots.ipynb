{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Across scenario basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../analysis/utils\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#helper function for pd.agg\n",
    "def item(x):\n",
    "    \"\"\"Returns representative single item\"\"\"\n",
    "    return x.tail(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up directory paths to plots and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes exported csvs from `basic_analysis.ipynb` in results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "studies = [\n",
    "    \"drop_pilot\",\n",
    "    \"collision_pilot\",\n",
    "    \"rollingsliding_pilot\",\n",
    "    \"dominoes_pilot\",\n",
    "    \"clothiness_pilot\",\n",
    "    \"towers_pilot\",\n",
    "    \"linking_pilot\",\n",
    "    \"containment_pilot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_accuracy-clothiness_pilot-production_2_testing.csv\r\n",
      "human_accuracy-collision_pilot-production_2_testing.csv\r\n",
      "human_accuracy-containment_pilot-production_2_testing.csv\r\n",
      "human_accuracy-dominoes_pilot-production_1_testing.csv\r\n",
      "human_accuracy-drop_pilot-production_2_testing.csv\r\n",
      "human_accuracy-linking_pilot-production_2_testing.csv\r\n",
      "human_accuracy-rollingsliding_pilot-production_2_testing.csv\r\n",
      "human_accuracy-towers_pilot-production_2_testing.csv\r\n",
      "human_responses-clothiness_pilot-production_2_testing.csv\r\n",
      "human_responses-collision_pilot-production_2_testing.csv\r\n",
      "human_responses-containment_pilot-production_2_testing.csv\r\n",
      "human_responses-dominoes_pilot-production_1_testing.csv\r\n",
      "human_responses-drop_pilot-production_2_testing.csv\r\n",
      "human_responses-linking_pilot-production_2_testing.csv\r\n",
      "human_responses-rollingsliding_pilot-production_2_testing.csv\r\n",
      "human_responses-towers_pilot-production_2_testing.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls `echo $csv_dir/'humans'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframes\n"
     ]
    }
   ],
   "source": [
    "#load all experiments as one dataframe\n",
    "prefix = \"human_responses-\"\n",
    "suffix = lambda s: \"-production_%d_testing\" % (1 if 'dominoes' in s else 2)\n",
    "make_fname = lambda s: prefix + s + suffix(s) + \".csv\"\n",
    "df = []\n",
    "for l in studies:\n",
    "    _df = pd.read_csv(os.path.join(csv_dir, 'humans', make_fname(l)))\n",
    "    if 'study' not in _df.columns:\n",
    "        _df = _df.assign(study=[l] * len(_df), axis=0)\n",
    "    df.append(_df)\n",
    "df = pd.concat(df)\n",
    "# df = pd.concat([pd.read_csv(os.path.join(csv_dir, 'humans', make_fname(l))) for l in studies])\n",
    "print(\"Loaded dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               drop_pilot\n",
       "1               drop_pilot\n",
       "2               drop_pilot\n",
       "3               drop_pilot\n",
       "4               drop_pilot\n",
       "               ...        \n",
       "15145    containment_pilot\n",
       "15146    containment_pilot\n",
       "15147    containment_pilot\n",
       "15148    containment_pilot\n",
       "15149    containment_pilot\n",
       "Name: study, Length: 120450, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['study']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save nice scenario name\n",
    "df['scenario'] = df['study'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sort by it\n",
    "df.sort_values('scenario',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#subtract stimulus presentation time from reaction time\n",
    "df['rt'] = df['rt'] - 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop reaction times longer than 10 seconds\n",
    "df['rt'] = df['rt'].apply(lambda x: x if x < 10000 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outcome_helper(correct,response):\n",
    "    response = response == \"YES\"\n",
    "    if correct and response: return \"hit\"\n",
    "    if not correct and response: return \"false_alarm\"\n",
    "    if correct and not response: return \"correct_rejection\"\n",
    "    if not correct and not response: return \"miss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encode response kind\n",
    "df['outcome'] = [outcome_helper(correct, response) for correct, response in zip(df['correct'],df['response'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hit                  46011\n",
       "correct_rejection    43379\n",
       "false_alarm          16846\n",
       "miss                 14214\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['response'] = df['response'] == \"YES\" #encode response as boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_stim_agg = df.groupby('stim_ID').agg({\n",
    "    'scenario' : lambda s: s.head(1),\n",
    "    'correct' : lambda cs: np.mean([1 if c == True else 0 for c in cs]),\n",
    "    'response' : 'mean',\n",
    "    'c' : 'count',\n",
    "    'stim_url' : lambda s: s.head(1),\n",
    "})\n",
    "per_stim_agg.sort_values('scenario',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(per_stim_agg, col=\"scenario\", height=6)\n",
    "g.map(sns.histplot, \"correct\", bins=40, binrange=[0,1])\n",
    "g.set_axis_labels(\"Correct/Total per Stimulus\",\"Count\")\n",
    "g.savefig(os.path.join(plot_dir,\"per_stim_16.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenario = \"dominoes\"\n",
    "\n",
    "g = sns.FacetGrid(per_stim_agg.query(\"scenario == @scenario\"), col=\"scenario\", height=2, aspect=2)\n",
    "g.map(sns.histplot, \"correct\", bins=40, binrange=[0,1])\n",
    "g.set_axis_labels(\"Correct/Total per Stimulus\",\"Count\")\n",
    "g.savefig(os.path.join(plot_dir,\"per_stim_\"+scenario+\".pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_stim_agg.query(\"scenario == @scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do people have a bias for yes or no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_person_agg = df.groupby('gameID').agg({\n",
    "    'scenario' : lambda s: s.head(1),\n",
    "    'correct' : lambda cs: np.mean([1 if c == True else 0 for c in cs]),\n",
    "    'response' : 'mean',\n",
    "    'c' : 'count',\n",
    "})\n",
    "per_person_agg.sort_values('scenario',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean correctness for the guesses of a single participant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(per_person_agg, col=\"scenario\", height=6)\n",
    "g.map(sns.histplot, \"correct\", bins=40, binrange=[0,1])\n",
    "g.set_axis_labels(\"Correct/Total per Participant\",\"Count\")\n",
    "g.savefig(os.path.join(plot_dir,\"per_subject_16.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Yes/No bias for each person? 1 corresponds to Yes. Perfect guesses every time would mean a response of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(per_person_agg, col=\"scenario\", height=6)\n",
    "g.map(sns.histplot, \"response\", bins=40, binrange=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby('scenario').agg({'response':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T21:47:04.118002Z",
     "iopub.status.busy": "2021-05-25T21:47:04.116065Z",
     "iopub.status.idle": "2021-05-25T21:47:04.995615Z",
     "shell.execute_reply": "2021-05-25T21:47:04.994720Z",
     "shell.execute_reply.started": "2021-05-25T21:47:04.117884Z"
    },
    "tags": []
   },
   "source": [
    "What is the distribution of reaction times? This does not include the 2500ms it takes for the stimulus to be shown, where the buttons become available only afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"scenario\", hue=\"correct\", height=6)\n",
    "g.map(sns.histplot, \"rt\", bins=40, binrange=[0,10000])\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"scenario\", hue=\"correct\", height=6)\n",
    "g.map(sns.violinplot, \"correct\",\"rt\", order=[True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"scenario\", hue=\"outcome\", height=6)\n",
    "g.map(sns.violinplot, \"outcome\",\"rt\", order=[\"hit\",\"correct_rejection\",\"miss\",\"false_alarm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the d-prime of the experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy & adversarial example cases\n",
    "> Easy, hard, chance, adversarial; reliable, noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_stims = 3\n",
    "\n",
    "for scenario in per_stim_agg['scenario'].unique():\n",
    "    print(\"*****\",scenario,\"*****\")\n",
    "    df_s = per_stim_agg[per_stim_agg['scenario'] == scenario]\n",
    "    df_s = df_s.sort_values('correct')\n",
    "    #get easy stims\n",
    "    selection = df_s[df_s['correct']>0.90].sample(num_stims)\n",
    "    print(\"---- Easy stims ----\")\n",
    "    display(selection)\n",
    "    print(list(selection['stim_url']),\"\\n\")\n",
    "    #get hard stims\n",
    "    selection = df_s[(df_s['correct']>0.70) & (df_s['correct']<0.80)].sample(num_stims)\n",
    "    print(\"---- Hard stims ----\")\n",
    "    display(selection)\n",
    "    print(list(selection['stim_url']),\"\\n\")\n",
    "    #get chance stims\n",
    "    selection = df_s[(df_s['correct']>0.45) & (df_s['correct']<0.55)].sample(num_stims)\n",
    "    print(\"---- Chance stims ----\")\n",
    "    display(selection)\n",
    "    print(list(selection['stim_url']),\"\\n\")\n",
    "    #get adversarial stims\n",
    "#     selection = df_s[df_s['correct']<0.20].sample(num_stims)\n",
    "    selection = df_s.head(num_stims)\n",
    "    print(\"---- Adversarial stims ----\")\n",
    "    display(selection)\n",
    "    print(list(selection['stim_url']),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['occluders'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving out relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df['sessionID'].unique()).to_csv(os.path.join(csv_dir,\"sessionIDs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
