{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of human behavior on single scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to:** \n",
    "* Apply preprocessing to human behavioral data\n",
    "* Visualize distribution and compute summary statistics over **human** physical judgments\n",
    "* Output CSV that can be re-loaded into R notebook for statistical modeling & fancy visualizations\n",
    "\n",
    "**This notebook depends on:**\n",
    "* Running `./generate_dataframes.py` (INTERNAL USE ONLY)\n",
    "* Running `./upload_results.py` (INTERNAL USE ONLY)\n",
    "* Running `./download_results.py` (PUBLIC USE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "\n",
    "sys.path.append('./analysis_helpers')\n",
    "from importlib import reload\n",
    "from analysis_helpers import *\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# seaborn plotting themes\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(analysis_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(analysis_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment_meta import *\n",
    "HEM = pd.DataFrame(NEURIPS2021_EXPS) # HEM = \"human experiment metadata\"\n",
    "HEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get paths to all human response data\n",
    "data_paths = [os.path.join(csv_dir,'humans',i) for i in os.listdir(os.path.join(csv_dir,'humans'))]\n",
    "resp_paths = [i for i in data_paths if i.split('/')[-1].split('-')[0]=='human_responses']\n",
    "assert len(resp_paths)==8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load in example dataframe\n",
    "exp_ind = 0\n",
    "d = pd.read_csv(resp_paths[exp_ind])\n",
    "scenarioName = resp_paths[exp_ind].split('/')[-1].split('-')[1].split('_')[0]\n",
    "\n",
    "## some utility vars\n",
    "colnames_with_variable_entries = [col for col in sorted(d.columns) if len(np.unique(d[col]))>1]\n",
    "colnames = ['gameID','trialNum','prolificIDAnon','stim_ID','response','target_hit_zone_label','correct','choices','rt']\n",
    "# colnames = ['gameID','trialNum','stim_ID','response','target_hit_zone_label','correct','choices','rt']\n",
    "\n",
    "## subset dataframe by colnames of interest\n",
    "_D = d[colnames]\n",
    "_D = _D.assign(scenarioName = scenarioName)\n",
    "\n",
    "## preprocess RTs (subtract 2500ms presentation time, log transform)\n",
    "_D = _D.assign(RT = _D['rt'] - 2500) \n",
    "_D = _D.assign(logRT = np.log(_D['RT']))\n",
    "_D = _D.drop(columns=['rt'],axis=1)\n",
    "\n",
    "## convert responses to boolean\n",
    "binary_mapper = {'YES':True, 'NO':False}\n",
    "_D = _D.assign(responseBool = _D['response'].apply(lambda x: binary_mapper[x]), axis=0)\n",
    "\n",
    "print('Currently analyzing the {} experiment.'.format(scenarioName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exclusion criteria (from `preregistration_neurips2021.md`)\n",
    "\n",
    " Data from an entire experimental session will be excluded if the responses:\n",
    " * contain a sequence with unusually long streak, defined as occurring less than 2.5% of the time under random responding\n",
    " * contain a sequence of at least 24 trials alternating \"yes\" and \"no\" responses\n",
    " * are correct for fewer than 4 out of 10 familiarization trials (i.e., 30% correct or lower)\n",
    " * the mean accuracy for that participant is below 3 standard deviations below the median accuracy across all participants \\for that scenario\n",
    " * the mean log-transformed response time for that participant is 3 standard deviations above the median log-transformed response time across all participants for that scenario\n",
    " \n",
    "Excluded sessions will be flagged. Flagged sessions will not be included in the main analyses. We will also conduct our planned analyses with the flagged sessions included to investigate the extent to which the outcomes of the main analyses change when these sessions are included. Specifically, we will fit a statistical model to all sessions and estimate the effect of a session being flagged on accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from analysis_helpers import *\n",
    "D = apply_exclusion_criteria(_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution and compute summary statistics over human physical judgments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Human accuracy across participants for each stimulus\n",
    "We will analyze accuracy for each stimulus by computing the proportion of correct responses across all participants who viewed that stimulus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Dacc = D.groupby('prolificIDAnon').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap_mean(D, col = 'correct', nIter=1000):\n",
    "    bootmean = []\n",
    "    for currIter in np.arange(nIter):\n",
    "        bootD = D.sample(n=len(D),random_state=currIter, replace=True)            \n",
    "        bootmean.append(np.mean(bootD[col].values))\n",
    "    return bootmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootmeans = bootstrap_mean(Dacc, col='correct', nIter=1000)\n",
    "lb = np.percentile(bootmeans,2.5)\n",
    "ub = np.percentile(bootmeans,97.5)\n",
    "print('The observed mean accuracy for {} is {}.'.format(scenarioName, np.round(Dacc['correct'].mean(),3)))\n",
    "print('95% CI: [{},{}].'.format(lb.round(3),ub.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy across stimuli for each participant\n",
    "We will analyze accuracy for each participant by computing the proportion of correct responses across all stimuli. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Dacc = D.groupby('stim_ID').agg({'correct':np.mean})\n",
    "h = sns.histplot(data=Dacc, x='correct', bins=30, stat='probability')\n",
    "t = plt.title('Accuracy distribution across stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-human consistency for each stimulus\n",
    "We will estimate human-human consistency for each stimulus by computing the proportion of responses that match the modal response for that stimulus (whether that modal response is correct or incorrect).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(np.unique(D.groupby('stim_ID').agg({'correct':'count'})['correct'].values))==1 # sanity check\n",
    "numSubs = np.unique(D.groupby('stim_ID').agg({'correct':'count'})['correct'].values)[0] # get num subs\n",
    "Dmode = D.groupby('stim_ID').agg({'responseBool':scipy.stats.mode}) # compute modal response\n",
    "propModalResponse = Dmode['responseBool'].apply(lambda x: x[1][0]).values / numSubs # get proportion modal response\n",
    "Dmode = Dmode.assign(propModalResponse = Dmode['responseBool'].apply(lambda x: x[1][0]).values / numSubs) \n",
    "Dcombined = Dacc.merge(Dmode, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = sns.histplot(data=Dmode, x='propModalResponse', bins=30, stat='probability')\n",
    "t = plt.title('Distribution of agreement among participants (similar to above, but not exactly the same)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-human consistency across stimuli (within scenario)\n",
    "We will analyze human-human consistency by computing the mean correlation between (binary) response vectors produced by each human participant across all stimuli within each scenario. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create response feature matrix (numSubs x numTrialsPerSub)\n",
    "D2 = D.sort_values(by=['prolificIDAnon','stim_ID']).reset_index(drop=True)\n",
    "numSubs = len(np.unique(D['prolificIDAnon'].values))\n",
    "numTrialsPerSub = int(len(D)/numSubs)\n",
    "respMat = np.reshape(D2['responseBool'].values, (numSubs,numTrialsPerSub)) \n",
    "\n",
    "## sanity check that the reshape operation happened correctly\n",
    "assert len([i for (i,j) in list(zip(respMat[0],D2[:150]['responseBool'].values)) if i!=j])==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get pairwise correlation distances\n",
    "dists = scipy.spatial.distance.pdist(respMat, metric='correlation')\n",
    "corrMat = scipy.spatial.distance.squareform(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## distribution of pairwise correlations between participant response vectors\n",
    "h = sns.histplot(corrMat[np.triu_indices(n=len(corrMat), k=1)])\n",
    "t = plt.title('pairwise correlations between participant response vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairwiseCorrs = corrMat[np.triu_indices(n=len(corrMat), k=1)]\n",
    "lb = np.percentile(pairwiseCorrs, 2.5)\n",
    "ub = np.percentile(pairwiseCorrs, 97.5)\n",
    "print('The median pairwise correlation is {}.'.format(np.percentile(pairwiseCorrs, 50).round(3)))\n",
    "print('95% CI: [{},{}].'.format(lb.round(3),ub.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## plot heatmap of correlation distances between participants\n",
    "reallyRun = False\n",
    "if reallyRun:\n",
    "    sns.heatmap(corrMat, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy as a function of stimulus attributes\n",
    "We will conduct exploratory analyses of human accuracy as a function of various scenario-specific stimulus attributes that varied across trials. We will examine those stimulus attributes that varied across stimuli within each scenario and explore the relationship between each individual attribute and human accuracy, as well as beetween linear combinations of them and human accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human accuracy by scenario\n",
    "We will fit human responses across all scenarios with a mixed-effects logistic regression model, including scenario as a fixed effect and participants and individual stimuli as random effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymer4.models import Lmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Lmer('correct ~ (1|prolificIDAnon) + (1|stim_ID)',data = D, family='binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other exploratory human behavioral analyses\n",
    "* We will explore the relation of demographic variables on the performance of participants: how does age, gender, and educational status relate to the overall accuracy of a subject?\n",
    "* We will additionally explore any potential left/right or yes/no response biases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
