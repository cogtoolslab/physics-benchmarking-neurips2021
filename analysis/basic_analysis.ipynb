{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Notebook for human physics benchmark experiments\n",
    "\n",
    "[Pregistration dominoes_pilot](https://github.com/cogtoolslab/human-physics-benchmarking/blob/master/experiments/dominoes_pilot/preregistration_dominoes_pilot.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = \"dominoes_pilot\"\n",
    "# study = \"towers_pilot\"\n",
    "bucket_name = 'human-physics-benchmarking-dominoes-pilot' #name of S3 bucket/stimuli collection\n",
    "# bucket_name = 'human-physics-benchmarking-towers-pilot' #name of S3 bucket/stimuli collection\n",
    "stim_version = 'example' #the version of the stimuli uploaded\n",
    "iterationName = 'run_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish connection to mongo\n",
    "First thing you need to do is to establish an ssh tunnel (aka remote port forwarding) to the server, so that requests to the mongodb can be made \"as if\" the mongodb server is running on your local computer. Run this from the command line before you begin data analysis if you plan to fetch data from mongo. Insert your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!`ssh -fNL 27017:127.0.0.1:27017 fbinder@cogtoolslab.org`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../analysis/utils\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pd.agg\n",
    "def item(x):\n",
    "    \"\"\"Returns representative single item\"\"\"\n",
    "    return x.tail(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up directory paths to plots and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv(os.path.join(proj_dir,'auth.txt'), header = None) # this auth.txt file contains the password for the sketchloop user. Place in repo folder\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = conn['human_physics_benchmarking']\n",
    "coll = db[study]\n",
    "\n",
    "stim_db = conn['stimuli']\n",
    "stim_coll = stim_db[bucket_name+'_'+stim_version]\n",
    "print('Iterations List:', coll.distinct('iterationName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many records?\n",
    "print('We have {} records in mongo.'.format(coll.estimated_document_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct tidy dataframe with game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe of served stims\n",
    "stim_df = pd.DataFrame(stim_coll.find({}))\n",
    "stim_df.set_index('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get experimental results\n",
    "df = coll.find({\n",
    "            'iterationName':iterationName\n",
    "#             'prolificID': {'$exists' : True},\n",
    "#             'studyID': {'$exists' : True},\n",
    "#             'sessionID': {'$exists' : True},\n",
    "})\n",
    "df = pd.DataFrame(df)\n",
    "df['button_pressed'] = pd.to_numeric(df['button_pressed'])\n",
    "# print('unique Prolific IDs:', len(df['prolificID'].unique()))\n",
    "print(\"Shape:\",df.shape)\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out which gameids are complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which gameids have completed all trials that were served to them? \n",
    "#Note that this will also exclude complete trials whose games aren't in the stim database anymore (ie if it has been dropped)\n",
    "complete_gameids = []\n",
    "\n",
    "for gameid in df['gameID'].unique():\n",
    "    #get the corresponding games\n",
    "    served_stim_ID = None\n",
    "    for stims_ID in stim_df.index:\n",
    "        if gameid in stim_df.iloc[stims_ID]['games']:\n",
    "            #great, we found our corresponding stim_ID\n",
    "            served_stim_ID = stims_ID\n",
    "    if served_stim_ID == None:\n",
    "        #we haven't found the stim_ID\n",
    "#         print(\"No recorded entry for game_ID in stimulus database:\",gameid)\n",
    "        continue\n",
    "    served_stims = stim_df.at[served_stim_ID,'stims']\n",
    "    #let's check if we can find an entry for each stim\n",
    "    found_empty = False\n",
    "    for stim_ID in [s['stim_ID'] for s in served_stims.values()]:\n",
    "        #check if we have an entry for that stimulus\n",
    "        if len(df.query(\"gameID == '\"+gameid+\"' & stim_ID == '\"+stim_ID+\"'\")) == 0:\n",
    "            found_empty = True\n",
    "            break\n",
    "    if not found_empty: complete_gameids.append(gameid)\n",
    "        \n",
    "print(\"Completed games:\",complete_gameids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mark unfinished entries\n",
    "df['complete_experiment'] = df['gameID'].isin(complete_gameids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many started games?\n",
    "print('We have {} unique games in mongo.'.format(len(df['gameID'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many completed games?\n",
    "print('We have {} unique completed games in mongo.'.format(len(df[df['complete_experiment']==True]['gameID'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude unfinished games ⚠️\n",
    "df = df[df['gameID'].isin(complete_gameids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some useful views\n",
    "df_trial_entries = df[(df['condition'] == 'prediction') & (df['trial_type'] == 'video-button-response')] #only experimental trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structures\n",
    "Let's look at the rating for the structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial_entries['c'] = 1 #add dummy variable for count in agg\n",
    "per_stim_agg = df_trial_entries.groupby('stim_ID').agg({\n",
    "    'correct' : lambda cs: np.mean([1 if c == True else 0 for c in cs]),\n",
    "    'c' : 'count',\n",
    "    'middle_objects' : item\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A view of the different structures\n",
    "per_stim_agg.sort_values('c',ascending=False).query(\"c > 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_stim_agg.to_csv(os.path.join(csv_dir,\"per_stim_agg_\"+study+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(per_stim_agg['correct'])\n",
    "plt.xlabel(\"Mean correctness per stimulus\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram mean correctness per stimulus on \"+study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjects\n",
    "Let's look at the distribution between subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_person_agg = df_trial_entries.groupby('gameID').agg({\n",
    "    'correct' : lambda cs: np.mean([1 if c == True else 0 for c in cs]),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram over average rate of correct guesses between **subjects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(per_person_agg['correct'])\n",
    "plt.xlabel(\"Mean correctness per subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram mean correctness per subject on \"+study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Let's look at some basic summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T21:25:56.729608Z",
     "iopub.status.busy": "2021-03-09T21:25:56.729522Z",
     "iopub.status.idle": "2021-03-09T21:25:56.731418Z",
     "shell.execute_reply": "2021-03-09T21:25:56.731261Z",
     "shell.execute_reply.started": "2021-03-09T21:25:56.729594Z"
    }
   },
   "source": [
    "### Overall correct on trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(round(df_trial_entries['correct'].mean() * 100,2)) + '% correct across all subjects and structures, excluding familiarization trials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tools_bc]",
   "language": "python",
   "name": "conda-env-tools_bc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
